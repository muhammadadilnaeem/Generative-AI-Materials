{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "# **Source Code Analysis Experimentation**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from git import Repo\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma  # Updated import\n",
    "from langchain.text_splitter import Language\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set up Environment Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OpenAI Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Clone Repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/entbappy/End-to-end-Medical-Chatbot-Generative-AI\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split Python Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loader instance to load files from the specified filesystem path\n",
    "loader = GenericLoader.from_filesystem(\n",
    "    repo_path,               # The path to the repository from which files will be loaded\n",
    "    glob=\"**/*\",            # A glob pattern to match all files recursively in the directory\n",
    "    suffixes=[\".py\"],       # Only include files with the .py suffix (Python files)\n",
    "    parser=LanguageParser(   # Specify the parser to use for processing the files\n",
    "        language=Language.PYTHON,  # Set the language for the parser to Python\n",
    "        parser_threshold=500       # Set a threshold for the parser, possibly limiting processing for large files\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents using the previously created loader instance\n",
    "document = loader.load()  # This calls the load method on the loader to retrieve the files and parse their content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY=os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medicalbot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check first line\n",
    "document[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Split Code Into Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of RecursiveCharacterTextSplitter to split text into manageable chunks\n",
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,  # Specify the language of the documents to be split (Python in this case)\n",
    "    chunk_size=500,            # Set the maximum size of each chunk to 500 characters\n",
    "    chunk_overlap=20           # Define the number of overlapping characters between consecutive chunks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the loaded document into smaller chunks using the defined text splitter\n",
    "texts = documents_splitter.split_documents(document)  # This method processes the document and returns a list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Initialize Embedding Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Store Data in Chromadb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector database using the Chroma class to store document embeddings\n",
    "vectordb = Chroma.from_documents(\n",
    "    texts,                     # A list of documents (texts) to be processed and converted into embeddings\n",
    "    embedding=embeddings,      # The embedding model to use for converting the texts into vector representations\n",
    "    persist_directory='./db'   # Directory where the vector database will be stored persistently\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1824\\4050888053.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1824\\1512909670.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of ConversationSummaryMemory to manage conversation history and summary\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,                  # The language model (llm) used for generating summaries of the conversation\n",
    "    memory_key=\"chat_history\", # A key used to identify the conversation history within the memory\n",
    "    return_messages=True      # Indicates whether to return the messages in the summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ConversationalRetrievalChain to facilitate question-answering with memory and retrieval capabilities\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm,                                         # The language model (llm) used for generating answers to questions\n",
    "    retriever=vectordb.as_retriever(            # Set up the retriever from the vector database for fetching relevant documents\n",
    "        search_type=\"mmr\",                      # Specify the search type; \"mmr\" stands for Maximum Marginal Relevance\n",
    "        search_kwargs={\"k\": 8}                  # Set additional search parameters; \"k\" defines the number of top results to return (in this case, 8)\n",
    "    ),\n",
    "    memory=memory                                # Pass the memory object to retain conversation context and history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings funtion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_1824\\79176006.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa(question)\n",
      "Number of requested results 20 is greater than number of elements in index 13, updating n_results = 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `download_hugging_face_embeddings` function is a function that downloads embeddings from the Hugging Face model \"sentence-transformers/all-MiniLM-L6-v2,\" which returns embeddings with 384 dimensions. These embeddings are used for natural language processing tasks like similarity calculations, clustering, or retrieval in the context of the application.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
