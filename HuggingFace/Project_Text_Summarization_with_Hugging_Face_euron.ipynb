{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"1c497159fbe345a7a6491771e4ce0e4c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1ff851e3ed604443aced51501606998e","IPY_MODEL_69d0823981f5405eb5aae9db1d75c966","IPY_MODEL_64459f43f4ea4114b78b6b80ef1435ba"],"layout":"IPY_MODEL_d1bf5ccfc01a403e9e430c29600d095c"}},"1ff851e3ed604443aced51501606998e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9928ae2aff3a41f3b2d5b42e47fe3d5c","placeholder":"​","style":"IPY_MODEL_95b54c32de864a23a7801e7209ea7a5d","value":"Map: 100%"}},"69d0823981f5405eb5aae9db1d75c966":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_830d6d7776df4d31b1f954044634d02c","max":818,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8929d1d00fbe48a7aa8a9779a5e7885f","value":818}},"64459f43f4ea4114b78b6b80ef1435ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_43347f54e74743e09ae632e344ea7d47","placeholder":"​","style":"IPY_MODEL_f34a5176580347d9901b968abc504176","value":" 818/818 [00:00&lt;00:00, 1155.74 examples/s]"}},"d1bf5ccfc01a403e9e430c29600d095c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9928ae2aff3a41f3b2d5b42e47fe3d5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95b54c32de864a23a7801e7209ea7a5d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"830d6d7776df4d31b1f954044634d02c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8929d1d00fbe48a7aa8a9779a5e7885f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"43347f54e74743e09ae632e344ea7d47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f34a5176580347d9901b968abc504176":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["\n","-----\n","\n","# **Text Summarization Using HuggingFace**\n","\n","- Install these libraries.\n","\n","```python\n","!pip install transformers[sentencepiece] datasets sacrebleu evaluate rouge_score py7zr -q\n","```\n","```python\n","!pip install --upgrade accelerate\n","!pip uninstall -y transformers accelerate\n","!pip install transformers accelerate\n","```\n","\n","Here's a brief overview of each library:\n","\n","1. **transformers**: This library by Hugging Face provides pre-trained models for natural language processing (NLP) tasks, such as text classification, translation, summarization, and more. It supports various architectures like BERT, GPT-2, and T5.\n","\n","2. **datasets**: Also from Hugging Face, this library allows easy access to a wide range of datasets for NLP tasks. It simplifies loading, preprocessing, and using datasets, making it easier to train and evaluate models.\n","\n","3. **sacrebleu**: This library is used for calculating BLEU scores, a metric for evaluating the quality of text that has been machine-translated from one language to another. It provides a standardized way to compute BLEU scores and includes various features for handling different text formats.\n","\n","4. **rouge_score**: This library is used for computing ROUGE scores, which are metrics for evaluating automatic summarization and machine translation. ROUGE measures the overlap between the generated text and reference text, focusing on recall, precision, and F1 scores.\n","\n","5. **py7zr**: This library is a Python implementation for handling 7z (7-Zip) archive files. It allows for the extraction and creation of compressed files, which can be useful for managing large datasets or models.\n","\n","6. **-q**: This flag is generally used with `pip` to suppress output messages, making the installation process quieter.\n","\n","These libraries are commonly used in NLP projects, particularly when working with model training and evaluation.\n","\n","\n","-----"],"metadata":{"id":"koTmfHGJgEMD"}},{"cell_type":"markdown","source":["### **1. Import Required Libraries**"],"metadata":{"id":"z47_J707lhw4"}},{"cell_type":"code","source":["# Import tqdm for creating progress bars in loops\n","from tqdm import tqdm\n","\n","# Import PyTorch for tensor computations and model handling\n","import torch\n","\n","# Import NLTK library for natural language processing tasks\n","import nltk\n","nltk.download(\"punkt\")\n","\n","# Import sentence tokenizer from NLTK\n","from nltk.tokenize import sent_tokenize\n","\n","# Import pandas for data manipulation and analysis\n","import pandas as pd\n","\n","# Import matplotlib for data visualization\n","import matplotlib.pyplot as plt\n","\n","# evaluate library and replacing load_metric with evaluate.load\n","import evaluate\n","\n","# Import Hugging Face Transformers library for using pre-trained models\n","from transformers import pipeline, set_seed\n","\n","# Import model and tokenizer classes for sequence-to-sequence learning\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","# Import datasets library for loading and manipulating datasets\n","from datasets import load_dataset , load_from_disk\n","\n","# Import DataCollatorForSeq2Seq from the Transformers library\n","# This class is used for dynamically padding sequences to the maximum length in a batch,\n","# making it suitable for sequence-to-sequence tasks during training or evaluation\n","from transformers import DataCollatorForSeq2Seq\n","\n","# Import TrainingArguments and Trainer from the Transformers library\n","# TrainingArguments is a class that holds various parameters for training (like learning rate, batch size, etc.)\n","# Trainer is a high-level class that simplifies the training and evaluation of models\n","from transformers import TrainingArguments, Trainer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vuj_KcpHlyNx","outputId":"43e99ad9-28d2-4ebd-c6ef-8710d0add4c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["#### **Check If gpu is available**"],"metadata":{"id":"Mcazjkcyoc_J"}},{"cell_type":"code","source":["# Check if a GPU (CUDA) is available for computation\n","# If a GPU is available, set the device to \"cuda\"; otherwise, use \"cpu\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Output the chosen device (either \"cuda\" or \"cpu\")\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"aieO_AUcnerK","outputId":"d3c1992c-74a2-4055-9394-06a019f5aa69"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["### **2. Load the Tokenizer Model**"],"metadata":{"id":"-WXwHS60ovLG"}},{"cell_type":"code","source":["# Specify the model checkpoint from Hugging Face's model hub\n","model_ckpt = \"google/pegasus-cnn_dailymail\"\n","\n","# Load the tokenizer associated with the specified model checkpoint\n","# The tokenizer is responsible for converting text into tokens that the model can process\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"],"metadata":{"id":"HyXBkamiolxG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **3. Load the pre-trained sequence-to-sequence model from the specified checkpoint**"],"metadata":{"id":"GGOGkBM5pitI"}},{"cell_type":"code","source":["# The model is set to the device (either GPU or CPU) for further computations\n","model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ejXHNXBo-ht","outputId":"75f6af70-d9c3-4115-ef1e-4817561c1ce6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["### **4. Load the SAMSum dataset, which is used for dialogue summarization tasks**"],"metadata":{"id":"08nVgfZDpxzI"}},{"cell_type":"code","source":["dataset_samsum = load_dataset(\"samsum\")"],"metadata":{"id":"bb2Urtjkppaf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_samsum"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V2DQjmlHp9dK","outputId":"288c86ef-d475-457e-df6a-1cfa33aad707"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 14732\n","    })\n","    test: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 819\n","    })\n","    validation: Dataset({\n","        features: ['id', 'dialogue', 'summary'],\n","        num_rows: 818\n","    })\n","})"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["# Check a Value\n","dataset_samsum[\"train\"][\"dialogue\"][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"LFiaXemjqGNk","outputId":"78820883-2eb2-480c-cf71-f585b38e23b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":54}]},{"cell_type":"markdown","source":["### **5. Create a list that contains the lengths of each split in the SAMSum dataset**"],"metadata":{"id":"5lf3pBOprCK_"}},{"cell_type":"code","source":["# This iterates through each split (e.g., train, validation, test) and calculates the length\n","split_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n","\n","# Print the lengths of each dataset split\n","print(f\"Split lengths: {split_lengths}\")\n","\n","# Print the feature names (column names) of the training set\n","print(f\"Features: {dataset_samsum['train'].column_names}\")\n","\n","# Print a header for the dialogue section\n","print(\"\\nDialogue:\")\n","\n","# Print the dialogue from the second entry (index 1) in the test set\n","print(dataset_samsum[\"test\"][1][\"dialogue\"])\n","\n","# Print a header for the summary section\n","print(\"\\nSummary:\")\n","\n","# Print the summary corresponding to the second entry (index 1) in the test set\n","print(dataset_samsum[\"test\"][1][\"summary\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CG4AQVKBqspx","outputId":"2ab33dc8-eb2b-4e12-f349-7b9e81910dd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Split lengths: [14732, 819, 818]\n","Features: ['id', 'dialogue', 'summary']\n","\n","Dialogue:\n","Eric: MACHINE!\r\n","Rob: That's so gr8!\r\n","Eric: I know! And shows how Americans see Russian ;)\r\n","Rob: And it's really funny!\r\n","Eric: I know! I especially like the train part!\r\n","Rob: Hahaha! No one talks to the machine like that!\r\n","Eric: Is this his only stand-up?\r\n","Rob: Idk. I'll check.\r\n","Eric: Sure.\r\n","Rob: Turns out no! There are some of his stand-ups on youtube.\r\n","Eric: Gr8! I'll watch them now!\r\n","Rob: Me too!\r\n","Eric: MACHINE!\r\n","Rob: MACHINE!\r\n","Eric: TTYL?\r\n","Rob: Sure :)\n","\n","Summary:\n","Eric and Rob are going to watch a stand-up on youtube.\n"]}]},{"cell_type":"markdown","source":["\n","### **6. Pre-Process the Dataset (Convert Example to Features)**\n","\n","- This function processes a batch of examples by tokenizing dialogues and summaries, preparing them for model input by creating appropriate input IDs, attention masks, and labels.\n"],"metadata":{"id":"OyUZ4b2Ar6cB"}},{"cell_type":"code","source":["def convert_examples_to_features(example_batch):\n","    # Encode the dialogues from the input batch using the tokenizer\n","    # Set the maximum length to 1024 tokens and enable truncation for longer texts\n","    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n","\n","    # Use the tokenizer configured for target text (summaries)\n","    with tokenizer.as_target_tokenizer():\n","        # Encode the summaries from the input batch\n","        # Set the maximum length to 128 tokens and enable truncation\n","        target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n","\n","    # Return a dictionary containing:\n","    # - 'input_ids': token IDs for the input dialogues\n","    # - 'attention_mask': mask indicating which tokens are actual tokens vs padding\n","    # - 'labels': token IDs for the target summaries (used for training)\n","    return {\n","        'input_ids': input_encodings['input_ids'],\n","        'attention_mask': input_encodings['attention_mask'],\n","        'labels': target_encodings['input_ids']\n","    }"],"metadata":{"id":"DFj_nmL1rRNg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","'''\n","-  Apply the 'convert_examples_to_features' function to the SAMSum dataset\n","-  This processes the dataset in batches to efficiently convert dialogues and summaries into model-ready features\n","-  The result is stored in 'dataset_samsum_pt' '''\n","dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["1c497159fbe345a7a6491771e4ce0e4c","1ff851e3ed604443aced51501606998e","69d0823981f5405eb5aae9db1d75c966","64459f43f4ea4114b78b6b80ef1435ba","d1bf5ccfc01a403e9e430c29600d095c","9928ae2aff3a41f3b2d5b42e47fe3d5c","95b54c32de864a23a7801e7209ea7a5d","830d6d7776df4d31b1f954044634d02c","8929d1d00fbe48a7aa8a9779a5e7885f","43347f54e74743e09ae632e344ea7d47","f34a5176580347d9901b968abc504176"]},"id":"eKIV5xx9sY-k","outputId":"8baedd97-a61a-4d52-feda-f7192c135bcd"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/818 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c497159fbe345a7a6491771e4ce0e4c"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["'''\n","-  Access the training split of the processed SAMSum dataset\n","-  This contains the features generated by the 'convert_examples_to_features' function\n","'''\n","\n","dataset_samsum_pt[\"train\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8A8QoaXztCkw","outputId":"f389ccc1-c76a-4a0b-bdb7-da6b8232a901"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n","    num_rows: 14732\n","})"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["# Access the input IDs of the second example (index 1) in the training split of the processed SAMSum dataset\n","# This retrieves the tokenized representation of the dialogue for that specific example\n","dataset_samsum_pt[\"train\"][\"input_ids\"][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sMozZd8atUHb","outputId":"dda32616-101f-4361-db5c-ac4cf237c77d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[18038,\n"," 151,\n"," 2632,\n"," 127,\n"," 119,\n"," 6228,\n"," 118,\n"," 115,\n"," 136,\n"," 2974,\n"," 152,\n"," 10463,\n"," 151,\n"," 35884,\n"," 130,\n"," 329,\n"," 107,\n"," 18038,\n"," 151,\n"," 2587,\n"," 314,\n"," 1242,\n"," 10463,\n"," 151,\n"," 1509,\n"," 1]"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["dataset_samsum_pt[\"train\"][\"labels\"][1]"],"metadata":{"id":"1dQXVB_ftjUI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4154171f-1c98-4056-8cdc-edec0dd57ccb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[18038, 111, 34296, 127, 6228, 118, 33195, 115, 136, 2974, 107, 1]"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["dataset_samsum_pt[\"train\"][\"attention_mask\"][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OTfjoO6LyzUJ","outputId":"2a26b407-6d60-45f2-ba02-cda60c8a5140"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["### **7. Train the model**\n","\n","- Create a data collator for sequence-to-sequence tasks using the specified tokenizer and model"],"metadata":{"id":"zF5q_MTvzl3E"}},{"cell_type":"code","source":["# This collator will pad inputs and labels dynamically when creating batches for training\n","seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\n","\n","# Define training arguments for the Trainer\n","trainer_args = TrainingArguments(\n","    # Directory where the model and checkpoints will be saved\n","    output_dir='pegasus-samsum',\n","\n","    # Number of training epochs\n","    num_train_epochs=1,\n","\n","    # Number of warmup steps for learning rate scheduling\n","    warmup_steps=500,\n","\n","    # Batch size for training on each device (e.g., GPU)\n","    per_device_train_batch_size=1,\n","\n","    # Batch size for evaluation on each device\n","    per_device_eval_batch_size=1,\n","\n","    # Weight decay for regularization\n","    weight_decay=0.01,\n","\n","    # How often to log training progress (in steps)\n","    logging_steps=10,\n","\n","    # Strategy for evaluating the model during training\n","    evaluation_strategy='steps',\n","\n","    # Steps between evaluations\n","    eval_steps=500,\n","\n","    # Steps at which the model checkpoint will be saved\n","    save_steps=1e6,\n","\n","    # Number of gradient accumulation steps (to effectively increase batch size)\n","    gradient_accumulation_steps=16\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R-gSx0x2y4Kd","outputId":"abab2559-1684-4a33-83bc-4aa91d1f8cf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Initialize the Trainer class with the specified parameters for training the model\n","trainer = Trainer(\n","    # The model to be trained (Pegasus model)\n","    model=model_pegasus,\n","\n","    # Training arguments defined earlier\n","    args=trainer_args,\n","\n","    # The tokenizer used for processing input and output sequences\n","    tokenizer=tokenizer,\n","\n","    # The data collator that handles dynamic padding of sequences\n","    data_collator=seq2seq_data_collator,\n","\n","    # The dataset to be used for training (in this case, using the test split)\n","    train_dataset=dataset_samsum_pt[\"test\"],\n","\n","    # The dataset to be used for evaluation during training (validation split)\n","    eval_dataset=dataset_samsum_pt[\"validation\"]\n",")"],"metadata":{"id":"dwfB2_jD0Caa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Start the training process for the model using the specified training parameters and datasets\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":165},"id":"PjfYX7mm0DRC","outputId":"c5f6d5cc-d073-43dd-c0d2-4b1f57c46adc"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [51/51 04:14, Epoch 0/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=51, training_loss=3.0044142264945832, metrics={'train_runtime': 257.2971, 'train_samples_per_second': 3.183, 'train_steps_per_second': 0.198, 'total_flos': 313450454089728.0, 'train_loss': 3.0044142264945832, 'epoch': 0.9963369963369964})"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["## **8. Evaluate the model Performance**"],"metadata":{"id":"tzcgZQH71Uqq"}},{"cell_type":"markdown","source":["#### **Calculate the ROUGE scores on a subset of the test dataset (first 10 examples)**"],"metadata":{"id":"10Owq4Im3B2C"}},{"cell_type":"code","source":["import torch\n","\n","def generate_batch_sized_chunks(list_of_elements, batch_size):\n","    \"\"\"split the dataset into smaller batches that we can process simultaneously\n","    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n","    for i in range(0, len(list_of_elements), batch_size):\n","        yield list_of_elements[i : i + batch_size]\n","\n","def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n","                               batch_size=16, device='cpu',\n","                               column_text=\"article\",\n","                               column_summary=\"highlights\"):\n","    # Move model to the specified device\n","    model.to(device)\n","\n","    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n","    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n","\n","    for article_batch, target_batch in tqdm(\n","        zip(article_batches, target_batches), total=len(article_batches)):\n","\n","        # Tokenize input articles\n","        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n","                        padding=\"max_length\", return_tensors=\"pt\")\n","\n","        # Move inputs to the same device as the model\n","        inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","        # Generate summaries with the model\n","        summaries = model.generate(input_ids=inputs[\"input_ids\"],\n","                                   attention_mask=inputs[\"attention_mask\"],\n","                                   length_penalty=0.8, num_beams=8, max_length=128)\n","\n","        # Decode the generated summaries\n","        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n","                                              clean_up_tokenization_spaces=True)\n","                             for s in summaries]\n","\n","        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n","\n","        # Add the generated summaries and targets to the metric\n","        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n","\n","    # Compute and return the ROUGE scores\n","    score = metric.compute()\n","    return score\n","\n","rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n","rouge_metric = evaluate.load('rouge')\n","\n","# Check if CUDA is available and use the GPU if possible, otherwise use CPU\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Evaluate the model's performance using the ROUGE metric\n","score = calculate_metric_on_test_ds(\n","    dataset_samsum['test'][0:10],  # Selecting the first 10 examples from the test dataset\n","    rouge_metric,                  # The ROUGE metric to be used for evaluation\n","    trainer.model,                 # The trained model to evaluate\n","    tokenizer,                     # The tokenizer used for processing the input\n","    batch_size=2,                  # Number of examples to process in each batch\n","    column_text='dialogue',        # The column name containing the text input (dialogue)\n","    column_summary='summary',      # The column name containing the reference summaries\n","    device=device                  # The device (GPU or CPU) to run the evaluation on\n",")\n","\n","# Create a dictionary to store the F1 scores for each ROUGE metric\n","rouge_dict = {}\n","for rn in rouge_names:\n","    if isinstance(score[rn], dict) and 'mid' in score[rn]:\n","        rouge_dict[rn] = score[rn]['mid'].fmeasure  # Handling case with 'mid' attribute\n","    else:\n","        rouge_dict[rn] = score[rn].fmeasure if hasattr(score[rn], 'fmeasure') else score[rn]  # Handling case where the score is a float\n","\n","# Convert the dictionary of ROUGE scores into a pandas DataFrame for easier viewing\n","pd.DataFrame(rouge_dict, index=[f'pegasus'])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":98},"id":"fAhIjH2F3EuK","outputId":"216a50e0-409b-403c-8dd7-ef6ff03c8f47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 5/5 [00:20<00:00,  4.09s/it]\n"]},{"output_type":"execute_result","data":{"text/plain":["           rouge1  rouge2    rougeL  rougeLsum\n","pegasus  0.022811     0.0  0.022521   0.022623"],"text/html":["\n","  <div id=\"df-25a280aa-1fe8-44e6-81e1-131bc796f782\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>rouge1</th>\n","      <th>rouge2</th>\n","      <th>rougeL</th>\n","      <th>rougeLsum</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>pegasus</th>\n","      <td>0.022811</td>\n","      <td>0.0</td>\n","      <td>0.022521</td>\n","      <td>0.022623</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25a280aa-1fe8-44e6-81e1-131bc796f782')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-25a280aa-1fe8-44e6-81e1-131bc796f782 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-25a280aa-1fe8-44e6-81e1-131bc796f782');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.022811295053074,\n        \"max\": 0.022811295053074,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.022811295053074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.02252134300668167,\n        \"max\": 0.02252134300668167,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.02252134300668167\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.022623369149321646,\n        \"max\": 0.022623369149321646,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.022623369149321646\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":68}]},{"cell_type":"markdown","source":["## **9. Save model**"],"metadata":{"id":"-DC-pVQi5zcP"}},{"cell_type":"code","source":["model_pegasus.save_pretrained(\"pegasus-samsum-model\")"],"metadata":{"id":"5TUNQNOx4C7e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Save tokenizer\n","tokenizer.save_pretrained(\"tokenizer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8jN36Ie456DB","outputId":"66b18413-e4bb-41c7-c813-314760568be4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/spiece.model',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"metadata":{},"execution_count":70}]},{"cell_type":"markdown","source":["## **10. Load the Model**"],"metadata":{"id":"KTPujQy95_QZ"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"],"metadata":{"id":"NdmPbKm658sv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **11. Prediction**"],"metadata":{"id":"-Mw6z-Gd6RYk"}},{"cell_type":"code","source":["gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\n","\n","sample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\n","\n","reference = dataset_samsum[\"test\"][0][\"summary\"]\n","\n","pipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n","\n","##\n","print(\"Dialogue:\")\n","print(sample_text)\n","\n","\n","print(\"\\nReference Summary:\")\n","print(reference)\n","\n","\n","print(\"\\nModel Summary:\")\n","print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E1hSTfHl6ONg","outputId":"c8434d54-2187-44d1-d48c-8b82a7b0a2bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n"]},{"output_type":"stream","name":"stdout","text":["Dialogue:\n","Hannah: Hey, do you have Betty's number?\n","Amanda: Lemme check\n","Hannah: <file_gif>\n","Amanda: Sorry, can't find it.\n","Amanda: Ask Larry\n","Amanda: He called her last time we were at the park together\n","Hannah: I don't know him well\n","Hannah: <file_gif>\n","Amanda: Don't be shy, he's very nice\n","Hannah: If you say so..\n","Hannah: I'd rather you texted him\n","Amanda: Just text him 🙂\n","Hannah: Urgh.. Alright\n","Hannah: Bye\n","Amanda: Bye bye\n","\n","Reference Summary:\n","Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n","\n","Model Summary:\n","Amanda: Ask Larry Amanda: He called her last time we were at the park together .<n>Hannah: I'd rather you texted him .<n>Amanda: Just text him .\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"eYkXvoZT6URu"},"execution_count":null,"outputs":[]}]}