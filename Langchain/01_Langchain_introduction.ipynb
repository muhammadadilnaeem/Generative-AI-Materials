{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rQxuRYluvkD"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **Langchain Introduction**\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DgepR14wq-B"
      },
      "source": [
        "## **LangChain**\n",
        "\n",
        "LangChain is a framework for developing applications powered by language models.\n",
        "\n",
        "- **GitHub**: https://github.com/hwchase17/langchain\n",
        "- **Docs**: https://python.langchain.com/v0.2/docs/introduction/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqtA9wmzxrub"
      },
      "source": [
        "## **Overview**\n",
        "\n",
        "- **Installation**\n",
        "- **LLMs**\n",
        "- **Prompt Templates**\n",
        "- **Chains**\n",
        "- **Agents and Tools**\n",
        "- **Memory**\n",
        "- **Document Loaders**\n",
        "- **Indexes**\n",
        "\n",
        "------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2j3ZyFAyAzi"
      },
      "source": [
        "### **Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IX3qawwLuk5Z",
        "outputId": "29c22331-ff84-4cc8-a9a2-8aafcbaf92f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.9)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.10 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.10)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.134)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwRdgp8U2utL"
      },
      "source": [
        "#### **Set Up OpenAI API Key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S--9TBdzy_2i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"YOUR_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ZRJ3ih3QYj"
      },
      "source": [
        "### **Large Language Models**\n",
        "\n",
        "\n",
        "- The basic building block of LangChain is a **Large Language Model** which takes text as input and generates more text\n",
        "\n",
        "- Suppose we want to generate a company name based on the company description, so we will first initialize an OpenAI wrapper. In this case, since we want the output to be more random, we will intialize our model with high temprature.\n",
        "\n",
        "- The temperature parameter adjusts the randomness of the output. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n",
        "\n",
        "- **temperature value**--> how creative we want our model to be\n",
        "\n",
        "- **0** ---> temperature it means model is very safe it is not taking any bets.\n",
        "\n",
        "- **1** --> it will take risk it might generate wrong output but it is very creative\n",
        "\n",
        "- A generic interface for all LLMs. See all LLM providers: https://python.langchain.com/docs/integrations/llms/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCKFEYDj4bcE"
      },
      "source": [
        "#### **1. OpenAI**\n",
        "\n",
        "- To use OpenAI Model We need to first Install langchain_openai and openai Library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ_eyppu4l3b",
        "outputId": "2d43b925-3e8b-4f45-ee46-a290d4074a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.51.2)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.10)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (0.1.134)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (24.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain_openai) (8.5.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.9->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain_openai) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4.0,>=0.3.9->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSFw7No6_kw1",
        "outputId": "28fdd4c3-803a-4352-dc3c-0de7a4166d1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Rainbow Threads LLC\n"
          ]
        }
      ],
      "source": [
        "# Import the OpenAI class from the langchain_openai module\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI model with a temperature setting of 0.9\n",
        "# The temperature controls the randomness of the output; higher values produce more diverse responses\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "# Define a prompt asking for a suggestion for a company name\n",
        "text = \"Suggest a name for a company that makes colorful socks.\"\n",
        "\n",
        "# Invoke the OpenAI model with the provided prompt and print the response\n",
        "print(llm.invoke(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn4DkjIqBB56"
      },
      "source": [
        "### **HuggingFace**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-qhQD8eAKD9",
        "outputId": "1c44573f-c36c-4f52-be1f-00814a78b38c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.3.10)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.39.0 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (4.44.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.1.134)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-huggingface) (8.5.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.39.0->langchain-huggingface) (0.4.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2024.8.30)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-huggingface) (2.23.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.0.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-huggingface) (1.2.2)\n",
            "Downloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers, langchain-huggingface\n",
            "Successfully installed langchain-huggingface-0.1.0 sentence-transformers-3.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface-hub langchain-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xGZ5oVR7B4Il",
        "outputId": "1f43d9cd-a658-4626-ca1c-b5edcc9d3de8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Wie alte sind Sie?'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the HuggingFaceHub class from the langchain_community.llms module\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize the HuggingFaceHub model with specific parameters\n",
        "# - repo_id specifies the model to be used (in this case, \"google/flan-t5-large\").\n",
        "# - model_kwargs is a dictionary that contains additional model parameters:\n",
        "#   - temperature: Controls the randomness of the output; set to 0 for deterministic results.\n",
        "#   - max_length: Limits the maximum length of the generated output; set to 64 tokens here.\n",
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\": 0, \"max_length\": 64})\n",
        "\n",
        "# Invoke the model with a translation task, translating from English to German\n",
        "# The input prompt is asking how to translate \"How old are you?\" into German.\n",
        "llm.invoke(\"translate English to German: How old are you?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMtHLbf2Dy92"
      },
      "source": [
        "### **Prompt Templates**\n",
        "\n",
        "- Currently in the above applications we are writing an entire prompt, if you are creating a user directed application then this is not an ideal case\n",
        "\n",
        "- LangChain faciliates prompt management and optimization.\n",
        "\n",
        "- Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM.\n",
        "\n",
        "- In many Large Language Model applications we donot pass the user input directly to the Large Language Model, we add the user input to a large piece of text called prompt template."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYDsFzqkC1Rs",
        "outputId": "3c223a0d-01a6-4807-d53c-d2634710397f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I want to open a restaurant for indian food. Suggest a fancy name for this.\n"
          ]
        }
      ],
      "source": [
        "# Import the PromptTemplate class from the langchain_core.prompts module\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Create an instance of PromptTemplate\n",
        "# - input_variables: A list of variables that will be used in the template; here, it includes 'cuisine'.\n",
        "# - template: The string template where '{cuisine}' will be replaced with the actual cuisine type.\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "# Format the prompt template by replacing the 'cuisine' variable with the string \"indian\"\n",
        "p = prompt_template_name.format(cuisine=\"indian\")\n",
        "\n",
        "# Print the formatted prompt\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "goGVwWc5Fn-T",
        "outputId": "47619d6c-9dd4-449f-9739-5e8cb0b5472e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'What is a good name for a company that makes colorful socks'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
        "\n",
        "prompt.format(product=\"colorful socks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1BsVgKkF5yb"
      },
      "source": [
        "### **Chains**\n",
        "\n",
        "- Combine LLMs and Prompts in multi-step workflows\n",
        "\n",
        "- Now as we have the model:\n",
        "\n",
        "```python\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
        "\n",
        "prompt.format(product=\"colorful socks\")\n",
        "```\n",
        "- Now using Chains we will link together model and the PromptTemplate and other Chains\n",
        "\n",
        "- The simplest and most common type of Chain is LLMChain, which passes the input first to Prompt Template and then to Large Language Model\n",
        "\n",
        "- LLMChain is responsible to execute the PromptTemplate, For every PromptTemplate we will specifically have an LLMChain.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xHLPkIIL04P",
        "outputId": "d7dac8b5-b320-4c28-8336-2afc2e82c5d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"Rainbow Threads Co.\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Example 1\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Initialize the LLM with the desired temperature\n",
        "llm = OpenAI(temperature=0.9)\n",
        "\n",
        "# Create the prompt template\n",
        "prompt = PromptTemplate.from_template(\"Give me a short, creative name for a company that makes {product}\")\n",
        "\n",
        "# Create the LLMChain\n",
        "chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "# Run the chain and get the response\n",
        "response = chain.run({\"product\": \"colorful socks\"})\n",
        "\n",
        "# Extract just the first line or a cleaned version of the response\n",
        "company_name = response.strip().split(\"\\n\")[0]  # In case it's a multiline response\n",
        "\n",
        "# Print the response\n",
        "print(company_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THKP8cQhPDLn"
      },
      "source": [
        "#### **To Visualize How Chains Work**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-_WVQz2MRYD",
        "outputId": "fdc6ac0d-b636-4886-81d1-ecc4e8087728"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Mexican food. Suggest a fancy name for this.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\"El Sabor Delicioso\" (The Delicious Flavor)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Initialize the LLM with the desired temperature\n",
        "llm = OpenAI()\n",
        "\n",
        "# Create the prompt template\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "# Create the LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name,verbose=True)\n",
        "\n",
        "# Run the chain and get the response\n",
        "response = chain.invoke({\"cuisine\": \"Mexican\"})\n",
        "\n",
        "# Extract the 'text' field from the response\n",
        "fancy_name = response['text'].strip()\n",
        "\n",
        "# Print the fancy name\n",
        "print(fancy_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9j9hAeiwRBFg"
      },
      "source": [
        "#### **Simple Sequential Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPItgmGSR3kr",
        "outputId": "f4f7ad63-a913-4d57-ee06-9cb620751949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for indian food. Suggest a fancy name for this.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSuggest some menu items for \n",
            "\n",
            "\"Spice Symphony\".\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1. Spicy Chicken Tikka Masala\n",
            "2. Lamb Vindaloo\n",
            "3. Paneer Butter Masala\n",
            "4. Tandoori Shrimp\n",
            "5. Vegetable Samosas\n",
            "6. Garlic Naan\n",
            "7. Aloo Gobi (potato and cauliflower curry)\n",
            "8. Chana Masala (chickpea curry)\n",
            "9. Palak Paneer (spinach and cheese curry)\n",
            "10. Mango Lassi (yogurt drink)\n",
            "11. Chicken Biryani\n",
            "12. Lamb Rogan Josh\n",
            "13. Prawn Curry\n",
            "14. Vegetable Korma\n",
            "15. Dal Makhani (creamy lentil curry)\n",
            "16. Tandoori Chicken\n",
            "17. Mixed Vegetable Pakoras\n",
            "18. Mango Chicken\n",
            "19. Malai Kofta (vegetable and cheese balls in creamy sauce)\n",
            "20. Rasmalai (Indian dessert made with cheese and saffron milk)\n"
          ]
        }
      ],
      "source": [
        "# Import necessary classes from Langchain modules\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI model with a temperature setting of 0.6\n",
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "# Create a prompt template for generating a restaurant name\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "# Create an LLMChain that combines the OpenAI model with the restaurant name prompt\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
        "\n",
        "# Create another prompt template for suggesting menu items based on the restaurant name\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables=['restaurant_name'],\n",
        "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
        ")\n",
        "\n",
        "# Create an LLMChain for generating menu items based on the restaurant name\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, verbose=True)\n",
        "\n",
        "# Combine the two chains into a sequential chain\n",
        "chain = SimpleSequentialChain(chains=[name_chain, food_items_chain])\n",
        "\n",
        "# Invoke the chain with the input \"indian\"\n",
        "response = chain.invoke(\"indian\")\n",
        "\n",
        "# Extract the 'output' part and clean it\n",
        "final_output = response['output'].strip()\n",
        "\n",
        "# Print the cleaned output\n",
        "print(final_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxyPlLn3UVHQ"
      },
      "source": [
        "#### **Sequential Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNFrHPllVzrs",
        "outputId": "0e82d8f9-72e5-478b-d0b0-13d852883dce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Pakistani food. Suggest a fancy name for this.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSuggest some menu items for \n",
            "\n",
            "\"Chandni Chowk Bistro\" .\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "cuisine : Pakistani\n",
            "restaurant_name : \"Chandni Chowk Bistro\"\n",
            "menu_items : 1. Butter Chicken: A classic North Indian dish of marinated chicken cooked in a creamy tomato-based sauce. \n",
            "\n",
            "2. Samosas: Crispy pastry pockets filled with spiced potatoes and peas, served with chutney for dipping. \n",
            "\n",
            "3. Tandoori Platter: A selection of tandoori grilled meats such as chicken tikka, seekh kebab, and tandoori shrimp. \n",
            "\n",
            "4. Palak Paneer: A vegetarian dish of creamy spinach and cottage cheese cubes, served with naan bread. \n",
            "\n",
            "5. Dal Makhani: A slow-cooked lentil dish simmered in a rich tomato and butter sauce. \n",
            "\n",
            "6. Aloo Paratha: Grilled flatbread stuffed with spiced potatoes and served with yogurt and chutney. \n",
            "\n",
            "7. Biryani: Fragrant basmati rice cooked with spices and a choice of chicken, lamb, or vegetables. \n",
            "\n",
            "8. Gol Gappe: Crispy hollow puris filled with a spiced potato mixture and tangy tamarind water. \n",
            "\n",
            "9. Chole Bhature: A popular street food dish of chickpeas cooked in a spicy onion and tomato gravy, served with fluffy fried bread. \n",
            "\n",
            "10. Mango Lassi: A refreshing\n"
          ]
        }
      ],
      "source": [
        "# Import necessary classes from Langchain modules\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI model\n",
        "llm = OpenAI(temperature=0.7)\n",
        "\n",
        "# Create prompt template for generating a restaurant name\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "# Create LLMChain for restaurant name generation\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\", verbose=True)\n",
        "\n",
        "# Create prompt template for generating menu items\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables=['restaurant_name'],\n",
        "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
        ")\n",
        "\n",
        "# Create LLMChain for menu items generation\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\", verbose=True)\n",
        "\n",
        "# Combine the chains in a sequential chain\n",
        "chain = SequentialChain(\n",
        "    chains=[name_chain, food_items_chain],\n",
        "    input_variables=['cuisine'],\n",
        "    output_variables=['restaurant_name', 'menu_items']\n",
        ")\n",
        "\n",
        "# Invoke the chain\n",
        "response = chain.invoke({\"cuisine\": \"Pakistani\"})\n",
        "\n",
        "# Clean the output by stripping unnecessary newlines or spaces\n",
        "restaurant_name = response['restaurant_name'].strip().replace('\\n', '')  # Removing extra newlines\n",
        "menu_items = response['menu_items'].strip()  # Clean any surrounding whitespace\n",
        "\n",
        "# Reformat the output in the desired human-readable format\n",
        "print(f\"cuisine : {response['cuisine']}\")\n",
        "print(f\"restaurant_name : {restaurant_name}\")\n",
        "print(f\"menu_items : {menu_items}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bQpR-fkYHPa"
      },
      "source": [
        "### **Agents and Tools**\n",
        "\n",
        "- Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "- When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
        "\n",
        "- **Tool**: A function that **performs a specific duty**. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "\n",
        "- **LLM**: The language model powering the agent.\n",
        "\n",
        "- **Agent**: The agent to use.\n",
        "\n",
        "- **Agent** is a very powerful concept in LangChain\n",
        "\n",
        "For example I have to travel from Dubai to Canada, I type this in ChatGPT\n",
        "\n",
        "---> Give me two **flight options** from Dubai to Canada on September 1, 2024 | ChatGPT will not be able to answer because has knowledge till September 2021\n",
        "\n",
        "- **ChatGPT plus** has **Expedia Plugin**, if we enable this plugin it will go to Expedia Plugin and will try to pull information about **Flights & it will show the information**.\n",
        "\n",
        "- **SerpApi** is a **real-time API** to access **Google search results**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYUP3LvAZMkq"
      },
      "source": [
        "##### **Wikepedia and llm-math Tool**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLHOMK2dXEl-",
        "outputId": "171bc892-1942-4a56-b10e-cd921cd98cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=1c3ba02149730bcbc908b193f67b094d442ea6257f8216b55267b69079d4bbea\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8es8CWuXgK3l",
        "outputId": "5ca63855-574f-4022-c130-d5409135c07d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the answer.\n",
            "Action: wikipedia\n",
            "Action Input: \"GDP of the US in 2024\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: List of U.S. states and territories by GDP\n",
            "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
            "The data source for the list is the Bureau of Economic Analysis (BEA) in 2024. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"\n",
            "Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ranking from one year to the next, even though they often make little or no difference in the standard of living of its population.\n",
            "Overall, in the calendar year 2024, the United States' Nominal GDP at Current Prices totaled at $28.269 trillion, as compared to $25.744 trillion in 2022.\n",
            "The three U.S. states with the highest GDPs were California ($3.987 trillion), Texas ($2.664 trillion), and New York ($2.226 trillion). The three U.S. states with the lowest GDPs were Vermont ($44.4 billion), Wyoming ($51.4 billion), and Alaska ($69.2 billion).\n",
            "GDP per capita also varied widely throughout the United States in 2024, with New York ($114,380), Massachusetts ($108,185), and North Dakota ($93,983) recording the three highest GDP per capita figures in the U.S., while Mississippi ($50,907), Arkansas ($58,449), and West Virginia ($57,857) recorded the three lowest GDP per capita figures in the U.S. The District of Columbia, though, recorded a GDP per capita figure far higher than any U.S. state in 2024 at $263,220.\n",
            "\n",
            "Page: List of countries by GDP (nominal) per capita\n",
            "Summary: The figures presented here do not take into account differences in the cost of living in different countries, and the results vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations change a country's ranking from one year to the next, even though they often make little or no difference to the standard of living of its population.\n",
            "GDP per capita is often considered an indicator of a country's standard of living; however, this is inaccurate because GDP per capita is not a measure of personal income.\n",
            "Comparisons of national income are also frequently made on the basis of purchasing power parity (PPP), to adjust for differences in the cost of living in different countries (see List of countries by GDP (PPP) per capita). PPP largely removes the exchange rate problem but not others; it does not reflect the value of economic output in international trade, and it also requires more estimation than GDP per capita. On the whole, PPP per capita figures are more narrowly spread than nominal GDP per capita figures.\n",
            "Non-sovereign entities (the world, continents, and some dependent territories) and states with limited international recognition are included in the list in cases in which they appear in the sources. These economies are not ranked in the charts here (except Kosovo and Taiwan), but are listed in sequence by GDP for comparison. In addition, non-sovereign entities are marked in italics.\n",
            "Four UN members (Cuba, Liechtenstein, Monaco and North Korea) do not belong to the International Monetary Fund (IMF), hence their economies are not ranked below. Kosovo, despite not being a member of the United Nations, is a member of IMF. Taiwan is not a IMF member but it is still listed in the official IMF indices.\n",
            "Several leading GDP-per-capita (nominal) jurisdictions may be considered tax havens, and their GDP data subject to material distortion by tax-planning activities. Examples include Bermuda, the Cayman Islands, Ireland and Luxembourg.\n",
            "All data are in current United States dollars. Historical data can be found here.\n",
            "\n",
            "Page: List of countries by GDP (nominal)\n",
            "Summary: Gross domestic product (GDP) is the market value of all final goods an\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now have multiple sources for the estimated GDP of the US in 2024.\n",
            "Action: Calculator\n",
            "Action Input: 28.269 trillion + 25.744 trillion\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 54013000000000.0\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: The estimated GDP of the US in 2024 is $54.013 trillion.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "{'input': 'What is the estimated GDP of the US in 2024 from Wikipedia?', 'output': 'The estimated GDP of the US in 2024 is $54.013 trillion.'}\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Initialize OpenAI with temperature 0 for factual responses\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Load the tools we want: Wikipedia for information retrieval and Calculator for math (if needed)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Initialize the agent with the loaded tools, the language model, and the zero-shot agent type\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Query the agent to get the estimated GDP of the US in 2024 using the Wikipedia tool\n",
        "response = agent.invoke(\"What is the estimated GDP of the US in 2024 from Wikipedia?\")\n",
        "\n",
        "# Print the result\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cIWZwThhlL0"
      },
      "source": [
        "### **Memory**\n",
        "\n",
        "- Chatbot application like ChatGPT, you will notice that it **remember past information**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv87LBZ8gawH",
        "outputId": "b177ebce-5d5d-412a-fe40-fffc6645d688"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for indian food. Suggest a fancy name for this.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSuggest some menu items for \n",
            "\n",
            "\"Maharaja's Kitchen\".\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1. Tandoori Chicken: Grilled chicken marinated in traditional Indian spices\n",
            "2. Butter Chicken: Tender chicken cooked in a creamy tomato-based sauce\n",
            "3. Rogan Josh: Lamb cooked in a rich, aromatic gravy\n",
            "4. Palak Paneer: Spinach and cottage cheese curry\n",
            "5. Dal Makhani: Slow-cooked black lentils in a creamy sauce\n",
            "6. Vegetable Biryani: Fragrant basmati rice layered with mixed vegetables and spices\n",
            "7. Chicken Tikka Masala: Grilled chicken in a spicy tomato and onion-based sauce\n",
            "8. Aloo Gobi: Cauliflower and potatoes cooked in a blend of spices\n",
            "9. Naan Bread: Freshly baked flatbread, perfect for dipping in curries\n",
            "10. Mango Lassi: A refreshing yogurt drink with mango puree and spices.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary classes from Langchain modules\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI model with a temperature setting of 0.6\n",
        "llm = OpenAI(temperature=0.6)\n",
        "\n",
        "# Create a prompt template for generating a restaurant name\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables=['cuisine'],\n",
        "    template=\"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
        ")\n",
        "\n",
        "# Create an LLMChain that combines the OpenAI model with the restaurant name prompt\n",
        "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n",
        "\n",
        "# Create another prompt template for suggesting menu items based on the restaurant name\n",
        "prompt_template_items = PromptTemplate(\n",
        "    input_variables=['restaurant_name'],\n",
        "    template=\"Suggest some menu items for {restaurant_name}.\"\n",
        ")\n",
        "\n",
        "# Create an LLMChain for generating menu items based on the restaurant name\n",
        "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, verbose=True)\n",
        "\n",
        "# Combine the two chains into a sequential chain\n",
        "chain = SimpleSequentialChain(chains=[name_chain, food_items_chain])\n",
        "\n",
        "# Invoke the chain with the input \"indian\"\n",
        "response = chain.invoke(\"indian\")\n",
        "\n",
        "# Extract the 'output' part and clean it\n",
        "final_output = response['output'].strip()\n",
        "\n",
        "# Print the cleaned output\n",
        "print(final_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBk-aMuh_BL",
        "outputId": "d39d067a-d43c-4fe6-8659-656fc74178dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mI want to open a restaurant for Pakistani food. Suggest a fancy name for this.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSuggest some menu items for \n",
            "\n",
            "\"Spice of Pakistan\" or \"Pakistani Delights\".\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "1. Chicken Biryani: A fragrant rice dish cooked with tender chicken, spices, and herbs.\n",
            "\n",
            "2. Tandoori Chicken: Marinated chicken cooked in a clay oven, served with a side of mint chutney.\n",
            "\n",
            "3. Seekh Kebab: Skewers of minced beef or lamb, seasoned with spices and grilled to perfection.\n",
            "\n",
            "4. Palak Paneer: Creamy spinach and cheese curry, a popular vegetarian dish in Pakistan.\n",
            "\n",
            "5. Haleem: A slow-cooked stew made with lentils, wheat, and meat, served with crispy fried onions and lemon.\n",
            "\n",
            "6. Naan Bread: Traditional flatbread, perfect for scooping up curries or kebabs.\n",
            "\n",
            "7. Aloo Gosht: Tender lamb or beef cooked with potatoes and spices, served with rice or naan.\n",
            "\n",
            "8. Chapli Kebab: Spicy beef or lamb patties, pan-fried and served with a side of yogurt sauce.\n",
            "\n",
            "9. Chicken Karahi: A delicious and spicy chicken curry cooked in a wok-style pan.\n",
            "\n",
            "10. Kheer: A creamy and sweet rice pudding dessert, flavored with cardamom and topped with nuts.\n"
          ]
        }
      ],
      "source": [
        "# Invoke the chain with the input \"indian\"\n",
        "response = chain.invoke(\"Pakistani\")\n",
        "\n",
        "# Extract the 'output' part and clean it\n",
        "final_output = response['output'].strip()\n",
        "\n",
        "# Print the cleaned output\n",
        "print(final_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW-9b3dkj084"
      },
      "outputs": [],
      "source": [
        "# Let's Check Memory\n",
        "chain.memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWctXxB9j7Uq",
        "outputId": "39a10da5-799b-4e4f-e557-cb3093eeb2b9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(chain.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTYynsmxj_jO"
      },
      "source": [
        "- We can Clearly see that no memory reference is Present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9BAUdmVkbaE"
      },
      "source": [
        "### **ConversationBufferMemory**\n",
        "\n",
        "- We can attach memory to remember all previous conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8g7OuH2tj-jo",
        "outputId": "03a648b3-98d5-4352-ac25-c1aaeb32c0a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"El Jardín de Sabores\" (The Garden of Flavors)\n"
          ]
        }
      ],
      "source": [
        "# Importing the ConversationBufferMemory class from the langchain.memory module\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Initializing an instance of ConversationBufferMemory to store conversation history\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Creating an LLMChain instance with a language model (llm), a prompt template (prompt_template_name),\n",
        "# and the memory instance to keep track of the conversation context\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "\n",
        "# Running the LLMChain with the input \"Mexican\" and storing the result in the variable 'name'\n",
        "name = chain.invoke(\"Mexican\")\n",
        "\n",
        "# Extracting and printing the output stored in the 'text' key of the 'name' dictionary\n",
        "print(name['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujE_ETZokYWu",
        "outputId": "adab4254-7608-4345-f601-f8f3948bf203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"Al-Amirah's Palace of Flavors\"\n"
          ]
        }
      ],
      "source": [
        "name = chain.invoke(\"Arabic\")\n",
        "print(name['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEX0Vu55ktcU",
        "outputId": "66c5e5a5-5aa6-41ae-874f-14a5b817a157"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Mexican\n",
            "AI: \n",
            "\n",
            "\"El Jardín de Sabores\" (The Garden of Flavors)\n",
            "Human: Arabic\n",
            "AI: \n",
            "\n",
            "\"Al-Amirah's Palace of Flavors\"\n"
          ]
        }
      ],
      "source": [
        "print(chain.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf36ULZol3bl"
      },
      "source": [
        "- As we Can see that our memory is being saved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jvyu_ZumQgF"
      },
      "source": [
        "\n",
        "### **ConversationChain**\n",
        "\n",
        "- Conversation buffer memory goes growing endlessly. If we want to remember last few conversations we can use **ConversationChain**.\n",
        "\n",
        "- Just remember last 5 Conversation Chain.\n",
        "\n",
        "- Just remember last 10-20 Conversation Chain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vupMLp5elwbp",
        "outputId": "1e273eb8-e404-4c21-b5b9-e07b45a16465"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=OpenAI(temperature=0.7))\n",
        "print(convo.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "p52cyVfqoKqb",
        "outputId": "dfa351c0-e358-45ab-fe24-a6c2849f08df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' The first cricket world cup was held in 1975 and was won by the West Indies team. The tournament was held in England and was a 60-over match. The West Indies team beat the Australia team in the final by 17 runs. Do you have any other questions about the cricket world cup?'"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"Who won the first cricket world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nag3cECarxZA",
        "outputId": "fda234ba-d110-4fcf-c955-767a71867427"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'  5+5 is equal to 10. Is there anything else you would like to know?'"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.run(\"How much is 5+5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmo2_nP6ruUJ",
        "outputId": "960e807d-bb8b-4008-cb75-ef519aa9c71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Who won the first cricket world cup?\n",
            "AI:  The first cricket world cup was held in 1975 and was won by the West Indies team. The tournament was held in England and was a 60-over match. The West Indies team beat the Australia team in the final by 17 runs. Do you have any other questions about the cricket world cup?\n",
            "Human: How much is 5+5?\n",
            "AI:   5+5 is equal to 10. Is there anything else you would like to know?\n"
          ]
        }
      ],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpsfRkpwsGLw"
      },
      "source": [
        "### **ConversationBufferWindowMemory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY28B9uXrwo2",
        "outputId": "d2cf9349-35bf-4288-8841-a100f43add7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Who won the first cricket world cup?',\n",
              " 'history': '',\n",
              " 'response': ' The first cricket world cup was held in 1975 and was won by the West Indies team.'}"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=3)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=OpenAI(temperature=0.7),\n",
        "    memory=memory\n",
        ")\n",
        "convo.invoke(\"Who won the first cricket world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15kS_RqOsWcU",
        "outputId": "cbf16277-f3c9-4c6e-e5bb-8eb68ad335bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Who was the captain of the winning team?',\n",
              " 'history': 'Human: Who won the first cricket world cup?\\nAI:  The first cricket world cup was held in 1975 and was won by the West Indies team.',\n",
              " 'response': ' The captain of the West Indies team during the 1975 cricket world cup was Clive Lloyd. He also captained the team during their second world cup win in 1979.'}"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "convo.invoke(\"Who was the captain of the winning team?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym7j-4pUsews",
        "outputId": "d6cf61ef-0ef3-45c0-8d38-5522bc963fa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: Who won the first cricket world cup?\n",
            "AI:  The first cricket world cup was held in 1975 and was won by the West Indies team.\n",
            "Human: Who was the captain of the winning team?\n",
            "AI:  The captain of the West Indies team during the 1975 cricket world cup was Clive Lloyd. He also captained the team during their second world cup win in 1979.\n"
          ]
        }
      ],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzqqBFonskcg"
      },
      "source": [
        "### **Document Loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8TJOKlUsnqh",
        "outputId": "a71bc822-634c-4a13-8556-a48a050dbe88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-5.0.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-5.0.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.5/294.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W--5_G8Nsph5"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Transformer_Models_and_BERT_Models.pdf\")\n",
        "pages = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzYbAZCBstAo",
        "outputId": "df20ba85-b511-4f38-9787-d413d5bd14d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 0}, page_content='Classify text with BERT\\nLearning Objectives\\n\\uf0b7Learn how to load a pre-trained BERT model from TensorFlow Hub\\n\\uf0b7Learn how to build your own model by combining with a classifier\\n\\uf0b7Learn how to train a BERT model by fine-tuning\\n\\uf0b7Learn how to save your trained model and use it\\n\\uf0b7Learn how to evaluate a text classification model\\nThis lab will show you how to fine-tune BERT to perform sentiment analysis on a dataset of plain-text\\nIMDB movie reviews. In addition to training a model, you will learn how to preprocess text into an \\nappropriate format.\\nBefore you start\\nPlease ensure you have a GPU (1 x NVIDIA Tesla T4 should be enough) attached to your Notebook \\ninstance to ensure that the training doesn\\'t take too long.\\nAbout BERT\\nBERT and other Transformer encoder architectures have been wildly successful on a variety of tasks \\nin NLP (natural language processing). They compute vector-space representations of natural \\nlanguage that are suitable for use in deep learning models. The BERT family of models uses the \\nTransformer encoder architecture to process each token of input text in the full context of all tokens \\nbefore and after, hence the name: Bidirectional Encoder Representations from Transformers.\\nBERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\\nimport os\\nimport warnings\\nwarnings.filterwarnings(\"ignore\")\\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\\nimport datetime\\nimport shutil\\nimport matplotlib.pyplot as plt\\nimport tensorflow as tf'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 1}, page_content='import tensorflow_hub as hub\\nimport tensorflow_text as text\\nfrom google.cloud import aiplatform\\nfrom official.nlp import optimization  # to create AdamW optmizer\\ntf.get_logger().setLevel(\"ERROR\")\\nTo check if you have a GPU attached. Run the following.\\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\\nSentiment Analysis\\nThis notebook trains a sentiment analysis model to classify movie reviews as positive or negative, \\nbased on the text of the review.\\nYou\\'ll use the Large Movie Review Dataset that contains the text of 50,000 movie reviews from the \\nInternet Movie Database.\\nDownload the IMDB dataset\\nLet\\'s download and extract the dataset, then explore the directory structure.\\nurl = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\\n# Set a path to a folder outside the git repo. This is important so data won\\'t get indexed by git on \\nJupyter lab\\npath = \"/home/jupyter/\"\\ndataset = tf.keras.utils.get_file(\\n    \"aclImdb_v1.tar.gz\", url, untar=True, cache_dir=path, cache_subdir=\"\"\\n)\\ndataset_dir = os.path.join(os.path.dirname(dataset), \"aclImdb\")\\ntrain_dir = os.path.join(dataset_dir, \"train\")'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 2}, page_content='# remove unused folders to make it easier to load the data\\nremove_dir = os.path.join(train_dir, \"unsup\")\\nshutil.rmtree(remove_dir)\\nNext, you will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset.\\nThe IMDB dataset has already been divided into train and test, but it lacks a validation set. Let\\'s \\ncreate a validation set using an 80:20 split of the training data by using the validation_split argument \\nbelow.\\nNote: When using the validation_split and subset arguments, make sure to either specify a random \\nseed, or to pass shuffle=False, so that the validation and training splits have no overlap.\\nAUTOTUNE = tf.data.AUTOTUNE\\nbatch_size = 32\\nseed = 42\\nraw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\\n    path + \"aclImdb/train\",\\n    batch_size=batch_size,\\n    validation_split=0.2,\\n    subset=\"training\",\\n    seed=seed,\\n)\\nclass_names = raw_train_ds.class_names\\ntrain_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\\nval_ds = tf.keras.preprocessing.text_dataset_from_directory(\\n    path + \"aclImdb/train\",\\n    batch_size=batch_size,\\n    validation_split=0.2,'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 3}, page_content='    subset=\"validation\",\\n    seed=seed,\\n)\\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\\ntest_ds = tf.keras.preprocessing.text_dataset_from_directory(\\n    path + \"aclImdb/test\", batch_size=batch_size\\n)\\ntest_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\\nLet\\'s take a look at a few reviews.\\nfor text_batch, label_batch in train_ds.take(1):\\n    for i in range(3):\\n        print(f\"Review: {text_batch.numpy()[i]}\")\\n        label = label_batch.numpy()[i]\\n        print(f\"Label : {label} ({class_names[label]})\")\\nLoading models from TensorFlow Hub\\nFor the purpose of this lab, we will be loading a model called Small BERT. Small BERT has the same \\ngeneral architecture as the original BERT but the has fewer and/or smaller Transformer blocks.\\nSome other popular BERT models are BERT Base, ALBERT, BERT Experts, Electra. See the continued \\nlearning section at the end of this lab for more info.\\nAside from the models available below, there are multiple versions of the models that are larger and \\ncan yeld even better accuracy but they are too big to be fine-tuned on a single GPU. You will be able \\nto do that on the Solve GLUE tasks using BERT on a TPU colab.\\nYou\\'ll see in the code below that switching the tfhub.dev URL is enough to try any of these models, \\nbecause all the differences between them are encapsulated in the SavedModels from TF Hub.\\nChoose a BERT model to fine-tune'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 4}, page_content='# defining the URL of the smallBERT model to use\\ntfhub_handle_encoder = (\\n    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\\n)\\n# defining the corresponding preprocessing model for the BERT model above\\ntfhub_handle_preprocess = (\\n    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\\n)\\nprint(f\"BERT model selected           : {tfhub_handle_encoder}\")\\nprint(f\"Preprocess model auto-selected: {tfhub_handle_preprocess}\")\\nPreprocessing model\\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before \\nbeing input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT \\nmodels discussed above, which implements this transformation using TF ops from the TF.text library. \\nIt is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\\nThe preprocessing model must be the one referenced by the documentation of the BERT model, \\nwhich you can read at the URL printed above. For BERT models from the drop-down above, the \\npreprocessing model is selected automatically.\\nNote: You will load the preprocessing model into a hub.KerasLayer to compose your fine-tuned \\nmodel. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model.\\nUse hub.KerasLayer to initialize the preprocessing\\nbert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\\nLet\\'s try the preprocessing model on some text and see the output:\\nCall the preprocess model function and pass text_test\\ntext_test = [\"this is such an amazing movie!\"]'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 5}, page_content='text_preprocessed = bert_preprocess_model(text_test)\\n# This print box will help you inspect the keys in the pre-processed dictionary\\nprint(f\"Keys       : {list(text_preprocessed.keys())}\")\\n# 1. input_word_ids is the ids for the words in the tokenized sentence\\nprint(f\\'Shape      : {text_preprocessed[\"input_word_ids\"].shape}\\')\\nprint(f\\'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}\\')\\n# 2. input_mask is the tokens which we are masking (masked language model)\\nprint(f\\'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}\\')\\n# 3. input_type_ids is the sentence id of the input sentence.\\nprint(f\\'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}\\')\\nAs you can see, now you have the 3 outputs from the preprocessing that a BERT model would use \\n(input_words_id, input_mask and input_type_ids).\\nSome other important points:\\nThe input is truncated to 128 tokens.\\nThe input_type_ids only have one value (0) because this is a single sentence input. For a multiple \\nsentence input, it would have one number for each input.\\nThe text pre-processor is a TensorFlow model. This means that instead of pre-processing separately, \\nwe can include it as a layer in the model code.\\nUsing the BERT model\\nBefore putting BERT into your own model, let\\'s take a look at its outputs. You will load it from TF Hub \\nand see the returned values.\\nbert_model = hub.KerasLayer(tfhub_handle_encoder)\\nbert_results = bert_model(text_preprocessed)'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 6}, page_content='print(f\"Loaded BERT: {tfhub_handle_encoder}\")\\nprint(f\\'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}\\')\\nprint(f\\'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}\\')\\nprint(f\\'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}\\')\\nprint(f\\'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}\\')\\nThe BERT models return a map with 3 important keys: pooled_output, sequence_output, \\nencoder_outputs:\\npooled_output to represent each input sequence as a whole. The shape is [batch_size, H]. You can \\nthink of this as an embedding for the entire movie review.\\nsequence_output represents each input token in the context. The shape is [batch_size, seq_length, \\nH]. You can think of this as a contextual embedding for every token in the movie review.\\nencoder_outputs are the intermediate activations of the L Transformer blocks. \\noutputs[\"encoder_outputs\"][i] is a Tensor of shape [batch_size, seq_length, 1024] with the outputs \\nof the i-th Transformer block, for 0 <= i < L. The last value of the list is equal to sequence_output.\\nFor the fine-tuning you are going to use the pooled_output array.\\nDefine your model\\nYou will create a very simple fine-tuned model, with the preprocessing model, the selected BERT \\nmodel, one Dense and a Dropout layer.\\nNote: for more information about the base model\\'s input and output you can use copy the model\\'s \\nurl to get to the documentation page.\\nThe order of the layers in the model will be:\\nInput Layer\\nPre-processing Layer\\nEncoder Layer\\nFrom the BERT output map, use pooled_output\\nDropout layer\\nDense layer with sigmoid activation\\ndef build_classifier_model(dropout_rate=0.1):'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 7}, page_content='    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text\")\\n    preprocessing_layer = hub.KerasLayer(\\n        tfhub_handle_preprocess, name=\"preprocessing\"\\n    )\\n    encoder_inputs = preprocessing_layer(text_input)\\n    encoder = hub.KerasLayer(\\n        tfhub_handle_encoder, trainable=True, name=\"BERT_encoder\"\\n    )\\n    outputs = encoder(encoder_inputs)\\n    net = outputs[\"pooled_output\"]\\n    net = tf.keras.layers.Dropout(dropout_rate)(net)\\n    net = tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"classifier\")(net)\\n    return tf.keras.Model(text_input, net)\\n# Let\\'s check that the model runs with the output of the preprocessing model.\\ndropout_rate = 0.15\\nclassifier_model = build_classifier_model(dropout_rate)\\nbert_raw_result = classifier_model(tf.constant(text_test))\\nprint(bert_raw_result)\\nThe output is meaningless, of course, because the model has not been trained yet.\\nLet\\'s take a look at the model\\'s structure.\\ntf.keras.utils.plot_model(classifier_model)\\nModel training\\nYou now have all the pieces to train a model, including the preprocessing module, BERT encoder, \\ndata, and classifier.\\nLoss function\\nSince this is a binary classification problem and the model outputs a probability (a single-unit layer), \\nyou\\'ll use losses.BinaryCrossentropy loss function.'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 8}, page_content='Define your loss and evaluation metrics here. Since it is a binary classification use BinaryCrossentropy\\nand BinaryAccuracy\\nloss = tf.keras.losses.BinaryCrossentropy()\\nmetrics = tf.metrics.BinaryAccuracy()\\nOptimizer\\nFor fine-tuning, let\\'s use the same optimizer that BERT was originally trained with: the \"Adaptive \\nMoments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight \\ndecay (not using moments), which is also known as AdamW.\\nIn past labs, we have been using the Adam optimizer which is a popular choice. However, for this lab \\nwe will be using a new optimizier which is meant to improve generalization. The intuition and \\nalgoritm behind AdamW can be found in this paper here.\\nFor the learning rate (init_lr), we use the same schedule as BERT pre-training: linear decay of a \\nnotional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps\\n(num_warmup_steps). In line with the BERT paper, the initial learning rate is smaller for fine-tuning \\n(best of 5e-5, 3e-5, 2e-5).\\nepochs = 5\\nsteps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\\nnum_train_steps = steps_per_epoch * epochs\\nnum_warmup_steps = int(0.1 * num_train_steps)\\ninit_lr = 3e-5\\noptimizer = optimization.create_optimizer(\\n    init_lr=init_lr,\\n    num_train_steps=num_train_steps,\\n    num_warmup_steps=num_warmup_steps,\\n    optimizer_type=\"adamw\",\\n)\\nLoading the BERT model and training'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 9}, page_content='Using the classifier_model you created earlier, you can compile the model with the loss, metric and \\noptimizer.\\nComplile the model using the optimizer, loss and metrics you defined above.\\nclassifier_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\\nNote: training time will vary depending on the complexity of the BERT model you have selected.\\nLet\\'s train the model for a few epochs.\\nprint(f\"Training model with {tfhub_handle_encoder}\")\\nhistory = classifier_model.fit(\\n    x=train_ds, validation_data=val_ds, epochs=epochs\\n)\\nEvaluate the model\\nLet\\'s see how the model performs. Two values will be returned. Loss (a number which represents the\\nerror, lower values are better), and accuracy.\\nloss, accuracy = classifier_model.evaluate(test_ds)\\nprint(f\"Loss: {loss}\")\\nprint(f\"Accuracy: {accuracy}\")\\nPlot the accuracy and loss over time\\nBased on the History object returned by model.fit(). You can plot the training and validation loss for \\ncomparison, as well as the training and validation accuracy:\\nhistory_dict = history.history\\nprint(history_dict.keys())\\nacc = history_dict[\"binary_accuracy\"]\\nval_acc = history_dict[\"val_binary_accuracy\"]\\nloss = history_dict[\"loss\"]'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 10}, page_content='val_loss = history_dict[\"val_loss\"]\\nepochs = range(1, len(acc) + 1)\\nfig = plt.figure(figsize=(10, 6))\\nfig.tight_layout()\\nplt.subplot(2, 1, 1)\\n# \"bo\" is for \"blue dot\"\\nplt.plot(epochs, loss, \"r\", label=\"Training loss\")\\n# b is for \"solid blue line\"\\nplt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\\nplt.title(\"Training and validation loss\")\\n# plt.xlabel(\\'Epochs\\')\\nplt.ylabel(\"Loss\")\\nplt.legend()\\nplt.subplot(2, 1, 2)\\nplt.plot(epochs, acc, \"r\", label=\"Training acc\")\\nplt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\\nplt.title(\"Training and validation accuracy\")\\nplt.xlabel(\"Epochs\")\\nplt.ylabel(\"Accuracy\")\\nplt.legend(loc=\"lower right\")\\nIn this plot, the red lines represents the training loss and accuracy, and the blue lines are the \\nvalidation loss and accuracy.\\nExport for inference\\nNow you just save your fine-tuned model for later use.\\nSave the model to saved_model_path for inference.'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 11}, page_content='dataset_name = \"imdb\"\\nsaved_model_path = \"./{}_bert\".format(dataset_name.replace(\"/\", \"_\"))\\nTIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\\nEXPORT_PATH = os.path.join(saved_model_path, TIMESTAMP)\\nclassifier_model.save(EXPORT_PATH, include_optimizer=False)\\nLet\\'s reload the model so you can try it side by side with the model that is still in memory.\\nreloaded_model = tf.saved_model.load(EXPORT_PATH)\\nHere you can test your model on any sentence you want, just add to the examples variable below.\\ndef print_my_examples(inputs, results):\\n    result_for_printing = [\\n        f\"input: {inputs[i]:<30} : score: {results[i][0]:.6f}\"\\n        for i in range(len(inputs))\\n    ]\\n    print(*result_for_printing, sep=\"\\\\n\")\\n    print()\\nexamples = [\\n    \"this is such an amazing movie!\",  # this is the same sentence tried earlier\\n    \"The movie was great!\",\\n    \"The movie was meh.\",\\n    \"The movie was okish.\",\\n    \"The movie was terrible...\",\\n]\\nreloaded_results = reloaded_model(tf.constant(examples))\\noriginal_results = classifier_model(tf.constant(examples))'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 12}, page_content='print(\"Results from the saved model:\")\\nprint_my_examples(examples, reloaded_results)\\nprint(\"Results from the model in memory:\")\\nprint_my_examples(examples, original_results)\\n(Optional) Deploy your model on Vertex AI to get online predictions\\nIf you want to use your model on TF Serving, remember that it will call your SavedModel through \\none of its named signatures. In Python, you can test them as follows:\\nserving_results = reloaded_model.signatures[\"serving_default\"](\\n    tf.constant(examples)\\n)\\nserving_results = serving_results[\"classifier\"]\\nprint_my_examples(examples, serving_results)\\nWe\\'ll export the model to a TensorFlow SavedModel format. Once we have a model in this format, \\nwe have lots of ways to \"serve\" the model, from a web application, from JavaScript, from mobile \\napplications, etc.\\nNext, print the signature of your saved model using the SavedModel Command Line Interface \\ncommand saved_model_cli. You can read more about the command line interface and the show and \\nrun commands it supports in the documentation here.\\n!saved_model_cli show \\\\\\n    --tag_set serve \\\\\\n    --signature_def serving_default \\\\\\n    --dir {EXPORT_PATH}\\n!find {EXPORT_PATH}\\nos.environ[\\'EXPORT_PATH\\'] = EXPORT_PATH\\nTIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\\nPROJECT = !gcloud config list --format \\'value(core.project)\\' 2>/dev/null'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 13}, page_content='PROJECT = PROJECT[0]\\nBUCKET = PROJECT\\nREGION = \"us-central1\"\\nMODEL_DISPLAYNAME = f\"classification-bert-{TIMESTAMP}\"\\nprint(f\"MODEL_DISPLAYNAME: {MODEL_DISPLAYNAME}\")\\n# from https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\\nSERVING_CONTAINER_IMAGE_URI = (\\n    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\"\\n)\\nos.environ[\"BUCKET\"] = BUCKET\\nos.environ[\"REGION\"] = REGION\\n%%bash\\n# Create GCS bucket if it doesn\\'t exist already...\\nexists=$(gsutil ls -d | grep -w gs://${BUCKET}/)\\nif [ -n \"$exists\" ]; then\\n    echo -e \"Bucket exists, let\\'s not recreate it.\"\\nelse\\n    echo \"Creating a new GCS bucket.\"\\n    gsutil mb -l ${REGION} gs://${BUCKET}\\n    echo \"\\\\nHere are your current buckets:\"\\n    gsutil ls\\nfi\\n!gsutil cp -r $EXPORT_PATH gs://$BUCKET/$MODEL_DISPLAYNAME\\nuploaded_model = aiplatform.Model.upload(\\n    display_name=MODEL_DISPLAYNAME,\\n    artifact_uri=f\"gs://{BUCKET}/{MODEL_DISPLAYNAME}\",\\n    serving_container_image_uri=SERVING_CONTAINER_IMAGE_URI,'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 14}, page_content=')\\nMACHINE_TYPE = \"n1-standard-4\"\\nendpoint = uploaded_model.deploy(\\n    machine_type=MACHINE_TYPE,\\n    accelerator_type=None,\\n    accelerator_count=None,\\n)\\nOnce the model has been uploaded to the endpoint, you can query the endpoint to get predictions \\nfrom the model. The query has to be a list of instances. From the signature of the model we see that \\nthe key for the query has to be text.\\ninstances = [\\n    {\"text\": [\"I loved the movie and highly recomment it\"]},\\n    {\"text\": [\"It was an okay movie in my opinion\"]},\\n    {\"text\": [\"I hated the movie\"]},\\n]\\nresponse = endpoint.predict(instances=instances)\\nprint(\" prediction:\", response.predictions)\\nCleanup\\nWhen deploying a model to an endpoint for online prediction, the minimum min-replica-count is 1, \\nand it is charged per node hour. So let\\'s delete the endpoint to reduce unnecessary charges. Before \\nwe can delete the endpoint, we first undeploy all attached models.\\nendpoint.undeploy_all()\\n...then delete the endpoint.\\nendpoint.delete()\\nContinued Learning\\nIn this lab, we chose small BERT to train our text classifier. There are other pre-trained BERT models \\nwhich you can find here. Consider experiementing with some of these. However, remember that the \\nbigger the model you choose to fine-tune, the longer it will take to train.'),\n",
              " Document(metadata={'source': '/content/Transformer_Models_and_BERT_Models.pdf', 'page': 15}, page_content='There are\\nBERT-Base, Uncased and seven more models with trained weights released by the original BERT \\nauthors.\\nSmall BERTs have the same general architecture but fewer and/or smaller Transformer blocks, which \\nlets you explore tradeoffs between speed, size and quality.\\nALBERT: four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by \\nsharing parameters between layers.\\nBERT Experts: eight models that all have the BERT-base architecture but offer a choice between \\ndifferent pre-training domains, to align more closely with the target task.\\nElectra has the same architecture as BERT (in three different sizes), but gets pre-trained as a \\ndiscriminator in a set-up that resembles a Generative Adversarial Network (GAN).\\nBERT with Talking-Heads Attention and Gated GELU [base, large] has two improvements to the core \\nof the Transformer architecture.\\nThe model documentation on TensorFlow Hub has more details and references to the research \\nliterature.\\nAside from the models available above, there are multiple versions of the models that are larger and \\ncan yeld even better accuracy but they are too big to be fine-tuned on a single GPU.\\nLicense\\nCopyright 2022 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may \\nnot use this file except in compliance with the License. You may obtain a copy of the License at \\nhttp://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in \\nwriting, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT \\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the \\nspecific language governing permissions and limitations under the License')]"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEAcyBJPsz6b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
