{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr6ojJjcc38H"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "# **Lnagchain With HuggingFace**\n",
        "\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxJ4LOPXdZ1G"
      },
      "source": [
        "## **Install Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlvDDixtcp9V",
        "outputId": "37d4a6f9-198c-4ec4-b5d2-8cee1203d44c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.34.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.9)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.134-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting sentence-transformers>=2.6.0 (from langchain-huggingface)\n",
            "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from langchain-huggingface) (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (24.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.4.1+cu121)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m733.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.2-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.134-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m255.2/255.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, requests-toolbelt, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, bitsandbytes, langsmith, sentence-transformers, langchain-core, langchain-text-splitters, langchain-huggingface, langchain, langchain-community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed bitsandbytes-0.44.1 dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.3 langchain-community-0.3.2 langchain-core-0.3.10 langchain-huggingface-0.1.0 langchain-text-splitters-0.3.0 langsmith-0.1.134 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 requests-toolbelt-1.0.0 sentence-transformers-3.2.0 tenacity-8.5.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-huggingface huggingface-hub transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0q3yHqpejK9"
      },
      "source": [
        "## **Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YQihWQVWei83"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Litlh24egdbc"
      },
      "source": [
        "## **Set Up HuggingFace API Key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jtZn9i2GfmH8"
      },
      "outputs": [],
      "source": [
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'API_KEY'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awZpSqCllUY1"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## **Approach 1: Access Models Hosted on Hugging Face Through API**\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpcNTaNpg8j7"
      },
      "source": [
        "## **Load a Model From HuggingFace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4b1mhmmkO4k"
      },
      "source": [
        "##### **Example 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d5Ux4xj3gmOc"
      },
      "outputs": [],
      "source": [
        "# Initialize a PromptTemplate object to create a prompt for generating company names\n",
        "prompt = PromptTemplate(\n",
        "    # Specify the input variable that will be used in the template\n",
        "    input_variables=[\"product\"],\n",
        "    # Define the template string that will be formatted with the input variable\n",
        "    template=\"What is a good name for a company that makes {product}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9elasyQhiYaE",
        "outputId": "75064fc8-0efb-41e0-a793-4dbd00a752b7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-a7c0e939fb8f>:4: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm=HuggingFaceHub(\n",
            "<ipython-input-5-a7c0e939fb8f>:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of LLMChain, which links a language model with a prompt\n",
        "chain = LLMChain(\n",
        "    # Specify the language model to use, sourced from Hugging Face Hub\n",
        "    llm=HuggingFaceHub(\n",
        "        # Define the repository ID for the specific model to be used\n",
        "        repo_id='google/flan-t5-large',\n",
        "        # Pass model-specific parameters in a dictionary\n",
        "        model_kwargs={\n",
        "            # Set the temperature to 0 for deterministic outputs (no randomness)\n",
        "            'temperature': 0,\n",
        "            # Limit the maximum length of the generated text to 64 tokens\n",
        "            'max_length': 64\n",
        "        }\n",
        "    ),\n",
        "    # Provide the previously defined prompt template to the chain\n",
        "    prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0LavS0viznZ",
        "outputId": "aeabd7ca-5315-4668-85dd-859fa2adc340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sock mania\n"
          ]
        }
      ],
      "source": [
        "# Invoke the LLMChain with a specific input to generate a response\n",
        "# In this case, the input is a string \"colorful socks\" which will be processed by the chain\n",
        "response = chain.invoke(\"colorful socks\")\n",
        "\n",
        "# Print the content of the 'text' key from the response dictionary\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R1kZCxUkayP"
      },
      "source": [
        "##### **Example 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UvIFkG9Zjxds"
      },
      "outputs": [],
      "source": [
        "# Initialize a PromptTemplate object to create a prompt for generating information about footballers\n",
        "prompt = PromptTemplate(\n",
        "    # Specify the input variable that will be used in the template\n",
        "    input_variables=[\"name\"],\n",
        "    # Define the template string that will be formatted with the input variable\n",
        "    template=\"Can you tell me about famous footballer {name}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PKXGddL2kmJu"
      },
      "outputs": [],
      "source": [
        "# Create an instance of LLMChain, which combines a language model with a prompt template\n",
        "chain = LLMChain(\n",
        "    # Specify the language model to use, sourced from the Hugging Face Hub\n",
        "    llm=HuggingFaceHub(\n",
        "        # Define the repository ID for the specific model to be used\n",
        "        repo_id='google/flan-t5-large',\n",
        "        # Provide model-specific parameters in a dictionary\n",
        "        model_kwargs={\n",
        "            # Set the temperature to 0 for deterministic outputs (no randomness)\n",
        "            'temperature': 0,\n",
        "            # Limit the maximum length of the generated text to 64 tokens\n",
        "            'max_length': 64\n",
        "        }\n",
        "    ),\n",
        "    # Provide the previously defined prompt template to the chain\n",
        "    prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX5o4RLskxxG",
        "outputId": "c4826213-82cd-4ed5-8c6e-c7a1875b0ff1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Messi is a footballer who plays for Argentina.\n"
          ]
        }
      ],
      "source": [
        "# Invoke the LLMChain with the input \"Messi\" to generate a response about the footballer\n",
        "response = chain.invoke(\"Messi\")\n",
        "\n",
        "# Print the content of the 'text' key from the response dictionary, which contains the generated information\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__fUBMeSlaZL"
      },
      "source": [
        "\n",
        "-----\n",
        "\n",
        "## **Apprach 01: Text Generation Models | Decoder Only Models**\n",
        "\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e6VCAk7llubI"
      },
      "outputs": [],
      "source": [
        "# Create a PromptTemplate object to generate a prompt for information about footballers\n",
        "prompt = PromptTemplate(\n",
        "    # Define the input variable that will be used in the prompt template\n",
        "    input_variables=[\"name\"],\n",
        "    # Specify the template string that will be formatted with the input variable\n",
        "    template=\"Can you tell me about famous footballer {name}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cdlEuMISmKl8"
      },
      "outputs": [],
      "source": [
        "# Create an instance of LLMChain to combine a language model with a prompt template\n",
        "chain = LLMChain(\n",
        "    # Specify the language model to use, sourced from the Hugging Face Hub\n",
        "    llm=HuggingFaceHub(\n",
        "        # Define the repository ID for the specific model to be utilized\n",
        "        repo_id='tiiuae/falcon-7b',\n",
        "        # Provide optional model-specific parameters in a dictionary\n",
        "        model_kwargs={\n",
        "            # Set the temperature to 0.1 to allow some randomness in the output\n",
        "            'temperature': 0.1,\n",
        "            # Limit the maximum length of the generated text to 64 tokens\n",
        "            'max_length': 64\n",
        "        }\n",
        "    ),\n",
        "    # Provide the previously defined prompt template to the chain\n",
        "    prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocIhQ9EDmXTw",
        "outputId": "7611ef93-b767-451a-f527-74c5eb038686"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Can you tell me about famous footballer Messi?\n",
            "Messi is a famous footballer. He is a player for the Argentina national team. He plays for the club Barcelona. He is the captain of the Argentina national team. He is the best footballer in the world. He is the best player in the world. He is the best player in the world. He is the best player in the world. He is the best player in the world. He is the best player in the world. He is the best player in the world\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke(\"Messi\")\n",
        "\n",
        "# Print the content of the 'text' key from the response dictionary, which contains the generated information\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esl2hQ0wmpOk"
      },
      "source": [
        "\n",
        "## **Approach 02: Download Model Locally (Create Pipelines)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6tfLwYZVmsZX"
      },
      "outputs": [],
      "source": [
        "# Import the PyTorch library for tensor operations and deep learning\n",
        "import torch\n",
        "\n",
        "# Import HuggingFacePipeline from LangChain for leveraging Hugging Face models\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "\n",
        "# Import the AutoTokenizer class for automatic loading of tokenizers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Import the AutoModelForCausalLM class for loading causal language models\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "# Import the pipeline function, which simplifies the use of Hugging Face models for various tasks\n",
        "from transformers import pipeline\n",
        "\n",
        "# Import the AutoModelForSeq2SeqLM class for loading sequence-to-sequence models\n",
        "from transformers import AutoModelForSeq2SeqLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxjHATZtnT_b"
      },
      "source": [
        "## **Load HuggingFace Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MSJnmUbznTMU"
      },
      "outputs": [],
      "source": [
        "# Define the identifier for the specific model to be used from Hugging Face's model hub\n",
        "model_id = 'google/flan-t5-large'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfwLr2FanYEO",
        "outputId": "b3fb007f-b49c-449c-e9e0-bb9460dd6a48"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer associated with the specified model from Hugging Face's model hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_id,                # Use the model ID defined earlier to specify which tokenizer to load\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_CJq5Fxn5Is",
        "outputId": "d2f508a0-c85b-48da-ee06-0e8ea3e965cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "# Load the sequence-to-sequence language model associated with the specified model ID from Hugging Face's model hub\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    model_id,                # Use the model ID defined earlier to specify which model to load\n",
        "    load_in_8bit=True,      # Load the model in 8-bit precision for reduced memory usage (deprecated)\n",
        "    device_map='auto'       # Automatically assign the model to available devices (e.g., GPU, CPU)\n",
        ")\n",
        "\n",
        "# Note: The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in future versions.\n",
        "# It is recommended to pass a `BitsAndBytesConfig` object in the `quantization_config` argument instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZopSfcKIo1Z2"
      },
      "source": [
        "## **Set up Text Generation Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Jrbn9UaQos9V"
      },
      "outputs": [],
      "source": [
        "# Create a pipeline for text-to-text generation using the specified model and tokenizer\n",
        "pipeline = pipeline(\n",
        "    \"text2text-generation\",  # Specify the task type for the pipeline, in this case, text-to-text generation\n",
        "    model=model,            # Pass the pre-trained model loaded earlier\n",
        "    tokenizer=tokenizer,    # Pass the tokenizer associated with the model\n",
        "    max_length=128          # Set the maximum length of the generated output to 128 tokens\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "O7r4mYyEo9P9"
      },
      "outputs": [],
      "source": [
        "# Create an instance of HuggingFacePipeline to wrap the previously defined pipeline for local use\n",
        "local_llm = HuggingFacePipeline(\n",
        "    pipeline=pipeline  # Pass the text-to-text generation pipeline created earlier\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQJLR72npa21"
      },
      "source": [
        "## **Set up Prompt Template**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MXlYx3mapFHF"
      },
      "outputs": [],
      "source": [
        "# Create a PromptTemplate object to generate a prompt for naming a company\n",
        "prompt = PromptTemplate(\n",
        "    # Define the input variable that will be used in the prompt template\n",
        "    input_variables=[\"product\"],\n",
        "    # Specify the template string that will be formatted with the input variable\n",
        "    template=\"What is a good name for a company that makes {product}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We7xUsbVpkV-"
      },
      "source": [
        "## **Set up LLMChain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HigIp70mpaPQ"
      },
      "outputs": [],
      "source": [
        "# Create an instance of LLMChain to link the language model with the prompt template\n",
        "chain = LLMChain(\n",
        "    # Specify the local language model to use, wrapped in HuggingFacePipeline\n",
        "    llm=local_llm,\n",
        "    # Provide the previously defined prompt template to guide the model's output\n",
        "    prompt=prompt\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMhwWUkNpnvJ",
        "outputId": "bb8888d9-4a75-4cea-d593-e5af2e4008b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sock mania\n"
          ]
        }
      ],
      "source": [
        "# In this case, the input is a string \"colorful socks\" which will be processed by the chain\n",
        "response = chain.invoke(\"colorful socks\")\n",
        "\n",
        "# Print the content of the 'text' key from the response dictionary\n",
        "print(response['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0_04GCrpuoc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
