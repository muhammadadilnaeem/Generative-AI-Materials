{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgMs13_0vPBO"
      },
      "source": [
        "# **Chatbot Implementation Using Langchain**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QEJH7_ZwOsk"
      },
      "source": [
        "### **Install Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_XHPuS_pu-vS"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain langchain-community langchain-openai openai pypdf sentence_transformers tiktoken faiss-cpu unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "ijFAANUH4HvD",
        "outputId": "bc9a8e3d-4c6f-4160-9e00-203d525f0461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Using cached numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "faiss-cpu 1.9.0 requires numpy<3.0,>=1.25.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f0fee9129f50404a8b8f32c3b95e8737",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.24.4\n",
        "!pip install nltk==3.9.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHjqWW9TwW7l"
      },
      "source": [
        "### **Load Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xf5fB8-5vvJm"
      },
      "outputs": [],
      "source": [
        "# Import the sys module for interacting with the interpreter and command-line arguments\n",
        "import sys\n",
        "\n",
        "# Import the os module for interacting with the operating system (e.g., file paths, environment variables)\n",
        "import os\n",
        "\n",
        "# Import the PyTorch library for tensor operations and deep learning\n",
        "import torch\n",
        "\n",
        "# Import the textwrap module for formatting text (e.g., wrapping long lines)\n",
        "import textwrap\n",
        "\n",
        "# Import UnstructuredURLLoader for loading documents from unstructured URLs\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "\n",
        "# Import CharacterTextSplitter to split text into smaller chunks based on character limits\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Import OpenAIEmbeddings for generating embeddings using OpenAI models\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Import ChatOpenAI to enable chat functionality with OpenAI's language models\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Import FAISS for efficient similarity search and clustering of dense vectors\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Import RetrievalQAWithSourcesChain for building a question-answering chain that retrieves sources\n",
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "# Import HuggingFaceEmbeddings for generating embeddings using Hugging Face models\n",
        "from langchain.embeddings import HuggingFaceEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "J3gmXg4bwpon",
        "outputId": "87bddc2d-cfc6-441e-d98a-e3197304b376"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "FileSystemPathPointer('/root/nltk_data/tokenizers/punkt')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the Natural Language Toolkit (nltk) library for natural language processing tasks\n",
        "import nltk\n",
        "\n",
        "# Download the 'punkt' tokenizer model, which is used for splitting text into sentences or words\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Download the 'averaged_perceptron_tagger' model, which is used for part-of-speech tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Ensure the punkt tokenizer is found\n",
        "nltk.data.find('tokenizers/punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMWupBHYxhov"
      },
      "source": [
        "### **Set Up API Key**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0H6AqZwyxmn-"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKGHJRBOxy3j"
      },
      "source": [
        "### **Paste the URLs and extract the data from these URLs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GTacMIozxvLd"
      },
      "outputs": [],
      "source": [
        "URLs=[\n",
        "    'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb',\n",
        "    'https://www.mosaicml.com/blog/mpt-7b',\n",
        "    'https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models',\n",
        "    'https://lmsys.org/blog/2023-03-30-vicuna/'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mVhYcKqyB-8"
      },
      "source": [
        "### **Load Website Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFfWhvlRx7AT",
        "outputId": "db081441-5a5f-4820-e28d-f842dfa75968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Create an instance of UnstructuredURLLoader to load documents from a list of URLs\n",
        "loaders = UnstructuredURLLoader(\n",
        "    urls=URLs  # Provide the list of URLs from which to load unstructured data\n",
        ")\n",
        "\n",
        "# Error handling for URL loading\n",
        "try:\n",
        "    # Load the data from the specified URLs using the loader\n",
        "    data = loaders.load()  # This method retrieves and processes the content from the URLs\n",
        "    print(\"Data loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FEIPi1FKyGE8"
      },
      "outputs": [],
      "source": [
        "# data #uncomment to see data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XF9ar2g2GcS",
        "outputId": "3ea3d92e-5a98-48dc-8a2b-8195a4c7cce5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data) # 4 because we loaded 4 web links"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVDJpvOc6Ea7"
      },
      "source": [
        "## **Create Chunks of Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wb97B-J25tTT"
      },
      "outputs": [],
      "source": [
        "# Create an instance of CharacterTextSplitter to split text into manageable chunks\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    # The character used to separate the text into chunks\n",
        "    separator='\\n',  # Split the text at newline characters\n",
        "    # The maximum size of each chunk in characters\n",
        "    chunk_size=1000,  # Each chunk will contain up to 1000 characters\n",
        "    # The number of overlapping characters between consecutive chunks\n",
        "    chunk_overlap=200  # Each chunk will overlap with the next by 200 characters\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5qKvBQvB6Hpr"
      },
      "outputs": [],
      "source": [
        "# Use the text_splitter instance to split the documents in 'data' into smaller chunks\n",
        "text_chunks = text_splitter.split_documents(data)\n",
        "\n",
        "# Explanation:\n",
        "# - data: This is expected to be a collection of documents (e.g., a list of strings or text objects) that you want to split into smaller, manageable pieces.\n",
        "# - text_chunks: This variable will hold the resulting list of text chunks after the split operation.\n",
        "# The split_documents method will apply the specified chunking logic (separator, chunk_size, and chunk_overlap) defined in the text_splitter instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlyhjkbB6ZmL",
        "outputId": "d50c55db-5f29-4122-c2fd-179e6eaf33c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of chunks: 86\n"
          ]
        }
      ],
      "source": [
        "# Get the number of text chunks created by the text_splitter\n",
        "number_of_chunks = len(text_chunks)\n",
        "print(f\"Number of chunks: {number_of_chunks}\")\n",
        "# Explanation:\n",
        "# - text_chunks: This variable contains the list of text chunks obtained from the previous split operation.\n",
        "# - len(text_chunks): This function returns the total number of chunks in the list.\n",
        "# The result is stored in 'number_of_chunks', which indicates how many individual text chunks were generated from the original documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxkI2_0u6h2b"
      },
      "source": [
        "### **Let's Check First Chunk**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIz3_6jh6aRo",
        "outputId": "a4217991-52ba-4451-ca69-a2437b093f7f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'https://blog.gopenai.com/paper-review-llama-2-open-foundation-and-fine-tuned-chat-models-23e539522acb'}, page_content='Open in app\\nSign up\\nSign in\\nWrite\\nSign up\\nSign in\\nPaper Review\\nPaper Review: Llama 2: Open Foundation and Fine-Tuned Chat Models\\nLlama 2: one of the best open source models\\nAndrew Lukyanenko\\nFollow\\nPublished in\\nGoPenAI\\n15 min read\\nJul 20, 2023\\n--\\nProject link\\nModel link\\nPaper link\\nThe authors of the work present Llama 2, an assortment of pretrained and fine-tuned large language models (LLMs) with sizes varying from 7 billion to 70 billion parameters. The fine-tuned versions, named Llama 2-Chat, are specifically designed for dialogue applications. These models surpass the performance of existing open-source chat models on most benchmarks, and according to human evaluations for usefulness and safety, they could potentially replace closed-source models. The authors also detail their approach to fine-tuning and safety enhancements for Llama 2-Chat to support the community in further developing and responsibly handling LLMs.\\nPretraining')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC5hZrQS60ET"
      },
      "source": [
        "## **Load OpenAI Embedding Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ZRr2dfbb6lri"
      },
      "outputs": [],
      "source": [
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LRymH8M6-1_",
        "outputId": "35f74db9-6e7e-4ba1-871a-51031052e030"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding length: 1536\n"
          ]
        }
      ],
      "source": [
        "# Generate an embedding for the query string \"Hello world\"\n",
        "query_result = embeddings.embed_query(\"Hello world\")\n",
        "\n",
        "# Get the length of the resulting embedding\n",
        "embedding_length = len(query_result)\n",
        "print(f\"Embedding length: {embedding_length}\")\n",
        "# Explanation:\n",
        "# - embeddings: This is an instance of an embedding model that transforms text into a numerical representation (embedding).\n",
        "# - embed_query(\"Hello world\"): This method takes the input query (in this case, \"Hello world\") and computes its embedding.\n",
        "# - query_result: This variable holds the resulting embedding, which is typically a list or array of numerical values representing the query.\n",
        "# - len(query_result): This function returns the total number of elements in the embedding.\n",
        "# The result is stored in 'embedding_length', which indicates the dimensionality of the embedding for the given query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO8-zUuk8k3s"
      },
      "source": [
        "## **Convert the Text Chunks into Embeddings and Create a Knowledge Base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zrXfGtXp7Pm5"
      },
      "outputs": [],
      "source": [
        "# Create a FAISS vector store from the text chunks and the embeddings model\n",
        "vectorstore = FAISS.from_documents(text_chunks, embeddings)\n",
        "\n",
        "# Explanation:\n",
        "# - FAISS: This refers to the Facebook AI Similarity Search library, which is used for efficient similarity search and clustering of dense vectors.\n",
        "# - from_documents(text_chunks, embeddings): This method initializes a FAISS vector store by processing the provided text chunks.\n",
        "#   - text_chunks: This is a list of text chunks that were previously split from the original documents.\n",
        "#   - embeddings: This is the embedding model used to convert the text chunks into dense vector representations.\n",
        "# - vectorstore: This variable holds the resulting FAISS vector store, which allows for efficient retrieval and similarity searches based on the embeddings of the text chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKVDwbzW9AkN"
      },
      "source": [
        "## **Create a Large Language Model (LLM) Wrapper**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kD0JMDu39CV5"
      },
      "outputs": [],
      "source": [
        "# Create an instance of the ChatOpenAI model for conversational AI tasks\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "# Explanation:\n",
        "# - ChatOpenAI: This refers to a class or function that initializes a language model designed for chat-based interactions, typically leveraging OpenAI's GPT architecture.\n",
        "# - llm: This variable holds the instance of the ChatOpenAI model, which can be used to generate responses, engage in dialogues, and perform various natural language processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjCEyWqT9Sar",
        "outputId": "2d999e59-4cbb-447e-d4ad-8c8e938acbb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Harry Potter is a popular fantasy book series written by J.K. Rowling that follows the life of a young wizard named Harry Potter as he attends Hogwarts School of Witchcraft and Wizardry. Throughout the series, Harry and his friends Ron and Hermione battle the dark wizard Lord Voldemort and his followers, the Death Eaters. The story explores themes of friendship, love, courage, and the power of good over evil.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 17, 'total_tokens': 99, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-03bf28e7-ae4d-4218-9697-7c435d23c3fc-0', usage_metadata={'input_tokens': 17, 'output_tokens': 82, 'total_tokens': 99, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"Please provide a concise summary of the Book Harry Potter\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtyFQFeo9wwb"
      },
      "source": [
        "## **Initialize the Retrieval QA with Source Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "89Mix05I9VCi"
      },
      "outputs": [],
      "source": [
        "# Create a RetrievalQAWithSourcesChain that combines the language model with a retriever\n",
        "chain = RetrievalQAWithSourcesChain.from_llm(\n",
        "    llm=llm,  # The ChatOpenAI instance used for generating responses\n",
        "    retriever=vectorstore.as_retriever()  # Convert the FAISS vector store to a retriever\n",
        ")\n",
        "\n",
        "# Explanation:\n",
        "# - RetrievalQAWithSourcesChain: This is a class that facilitates a question-answering system with retrieval capabilities, allowing it to fetch relevant documents based on a query before generating a response.\n",
        "# - from_llm(): This method initializes the chain using the specified language model and retriever.\n",
        "#   - llm: The ChatOpenAI instance that will be used to generate answers based on the retrieved documents.\n",
        "#   - retriever: This converts the FAISS vector store into a retriever interface, enabling the chain to search for relevant text chunks based on input queries.\n",
        "# - chain: This variable holds the instance of the RetrievalQAWithSourcesChain, which can process questions, retrieve documents, and generate answers that include sources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXAS9s7t986v",
        "outputId": "bce6c965-2b4d-4f63-b9f6-061e08c597d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-6dc5d9fb268a>:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  result=chain({\"question\": \"How good is Vicuna?\"}, return_only_outputs=True)\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_openai/chat_models/base.py:356: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
            "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
          ]
        }
      ],
      "source": [
        "result=chain({\"question\": \"How good is Vicuna?\"}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0NQnmboW-MKm",
        "outputId": "8543f9fc-796e-4d8e-c9a9-7124b030a3f1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Vicuna is considered to be very good, achieving high quality comparable to ChatGPT, OpenAI ChatGPT, and Google Bard, and outperforming models like LLaMA and Stanford Alpaca in over 90% of cases.\\n'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['answer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzeMNbKt-fPp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
