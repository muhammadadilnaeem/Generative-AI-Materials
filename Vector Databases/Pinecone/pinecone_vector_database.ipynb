{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFLh-pCCFYkb"
      },
      "source": [
        "-----\n",
        "\n",
        "# **Pinecone Vector Database**\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulmdrZmRFioD"
      },
      "source": [
        "## **1. Install Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VJdvXR1F4iL",
        "outputId": "98a23607-ad28-48a4-86b2-de414b9aba0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (5.3.1)\n",
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.51.2)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.2)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: langchain_pinecone in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (5.0.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2024.8.30)\n",
            "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (1.1.0)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.9 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.10)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.134)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.5.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.13.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.3->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.9->langchain-openai) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.9->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pinecone pinecone-client openai langchain-openai langchain-community langchain_pinecone pypdf tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHeMsmD7G2iF"
      },
      "source": [
        "## **2. Import Required Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "qzT_Bfw_GJof"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_openai import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM4UFMP4Iv3H"
      },
      "source": [
        "## **3. Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ca5dcNC_IOtr"
      },
      "outputs": [],
      "source": [
        "# Create directory to store data\n",
        "!mkdir PDF_DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb96Aa67JPrb",
        "outputId": "96e9080c-a548-4b37-e0ac-9b51c2faf038"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE\n",
            "To: /content/PDF_DATA/yolov7paper.pdf\n",
            "100% 2.27M/2.27M [00:00<00:00, 137MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vILwiv6nS2wI3chxNabMgry3qnV67TxM\n",
            "To: /content/PDF_DATA/rachelgreecv.pdf\n",
            "100% 271k/271k [00:00<00:00, 91.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Download the YOLOv7 research paper from Google Drive using gdown\n",
        "# The file is saved as 'yolov7paper.pdf' in the 'PDF_DATA' directory\n",
        "!gdown 1hPQlXrX8FbaYaLypxTmeVOFNitbBMlEE -O PDF_DATA/yolov7paper.pdf\n",
        "\n",
        "# Download Rachel Green's CV from Google Drive using gdown\n",
        "# The file is saved as 'rachelgreecv.pdf' in the 'PDF_DATA' directory\n",
        "!gdown 1vILwiv6nS2wI3chxNabMgry3qnV67TxM -O PDF_DATA/rachelgreecv.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8WyfaWoJ6b0"
      },
      "source": [
        "## **4. Extract Text From PDF_DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "cQwEENDVKF_a"
      },
      "outputs": [],
      "source": [
        "# Initialize a PyPDFDirectoryLoader instance to load PDF files from a specified directory\n",
        "# \"PDF_DATA/\" is the directory containing the PDF files to be loaded\n",
        "loader = PyPDFDirectoryLoader(\"PDF_DATA/\")\n",
        "\n",
        "# Load the PDF documents from the specified directory using the loader instance\n",
        "# This will read and parse the PDF files, returning them as a list of document objects\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "u0aXW6cdKfhD"
      },
      "outputs": [],
      "source": [
        "# documents  # Uncomment to see the extracted data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2H76CboK2E8"
      },
      "source": [
        "## **5. Convert Extracted Data Into Text Chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "lh1Sx2ZiKl5q"
      },
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8v-oRVCLXSt",
        "outputId": "a073ddc7-9282-44f4-dd4a-c4ba3309bb4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'PDF_DATA/yolov7paper.pdf', 'page': 0}, page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known')"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's Checkfirst chunk\n",
        "text_chunks[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY2LSVwyMMM0"
      },
      "source": [
        "## **6. Set Up OpenAI Key and Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "t9pZrNs6LeRm"
      },
      "outputs": [],
      "source": [
        "# Set up OpenAI API Key\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "rEUPRqABMnJX"
      },
      "outputs": [],
      "source": [
        "# Set up OpenAI Embeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "yBPzwRgFMxD1"
      },
      "outputs": [],
      "source": [
        "# Let's try to embed a text\n",
        "\n",
        "res = embeddings.embed_query(\"Hello Adil!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvjKrS46Myi6",
        "outputId": "66939d4b-517c-4855-ff65-d24e73cbd281"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1536"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# By default the length of OpenAI Embedding Vector Will be 1536\n",
        "\n",
        "len(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbWfh8iBOzna"
      },
      "source": [
        "## **6. Set Up Pincone API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "7b-S3rLDNXpm"
      },
      "outputs": [],
      "source": [
        "# API key to initialize your client\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=\"YOUR_API_KEY\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "OkNSvBz9c9Pa"
      },
      "outputs": [],
      "source": [
        "# Set the API key as an environment variable\n",
        "os.environ[\"PINECONE_API_KEY\"] = \"YOUR_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "8tSvoEaCPLjo"
      },
      "outputs": [],
      "source": [
        "# Create a serverless index\n",
        "\n",
        "# - An index defines the dimension of vectors to be stored and the similarity metric to be used when querying them.\n",
        "\n",
        "# - Create a serverless index with a dimension and similarity metric based on the embedding model you’ll use to create the vector embeddings:\n",
        "\n",
        "index_name = \"pineconepractice\"\n",
        "\n",
        "pc.create_index(\n",
        "    name=index_name,\n",
        "    dimension=1536, # Replace with your model dimensions\n",
        "    metric=\"cosine\", # Replace with your model metric\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",\n",
        "        region=\"us-east-1\"\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9wZe17hQjXS"
      },
      "source": [
        "## **7. Create Embeddings From Chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Rp6y-3-ocuXh"
      },
      "outputs": [],
      "source": [
        "# Create a Pinecone vector store instance from a list of text chunks\n",
        "docsearch = PineconeVectorStore.from_texts(\n",
        "    # Extract the page content from each text chunk and create a list\n",
        "    [t.page_content for t in text_chunks],\n",
        "\n",
        "    # Use OpenAI's embedding model to convert text into vector representations\n",
        "    embedding=OpenAIEmbeddings(),\n",
        "\n",
        "    # Specify the name of the index in Pinecone where the vectors will be stored\n",
        "    index_name=index_name,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDhOJP4nX45i",
        "outputId": "828b8b0f-b6a4-48eb-e26b-7c9795c395b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7b08afe2cb80>"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaUGJ0GBizyF"
      },
      "source": [
        "#### **Load the Existing Pinecone Index**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NaAbZHqZ0q4",
        "outputId": "e0315a23-b4e4-4fb3-e818-f2386d55cfc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x7b08e0788220>"
            ]
          },
          "execution_count": 105,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docsearch = PineconeVectorStore.from_existing_index(index_name, embeddings)\n",
        "docsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyfoa7GigY9c"
      },
      "source": [
        "## **8. Let's Perform Similarity Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "WG56o0HOgKVN"
      },
      "outputs": [],
      "source": [
        "query = \"YOLOv7 outperforms which models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCxZPpyGgqnK",
        "outputId": "b9baf650-cbf2-4039-ca5b-22063e355d6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(id='69ef3d73-31c3-4d1c-be2c-cc053cbb60b6', metadata={}, page_content='YOLOv7-tiny 6.2 3.5 320 30.8% 47.3% 32.2% 10.0% 31.9% 52.2%\\nimprovement -39% -49% - = = = -0.9 = +0.7\\nYOLOR-E6 [81] 115.8M 683.2G 1280 55.7% 73.2% 60.7% 40.1% 60.4% 69.2%\\nYOLOv7-E6 97.2M 515.2G 1280 55.9% 73.5% 61.1% 40.6% 60.3% 70.0%\\nimprovement -19% -33% - +0.2 +0.3 +0.4 +0.5 -0.1 +0.8\\nYOLOR-D6 [81] 151.7M 935.6G 1280 56.1% 73.9% 61.2% 42.4% 60.5% 69.9%\\nYOLOv7-D6 154.7M 806.8G 1280 56.3% 73.8% 61.4% 41.3% 60.6% 70.1%\\nYOLOv7-E6E 151.7M 843.2G 1280 56.8% 74.4% 62.1% 40.8% 62.1% 70.6%'),\n",
              " Document(id='eb6b8f9b-ff28-4621-9f9d-e830932b0b34', metadata={}, page_content='YOLOv5-L6 (r6.1) [23] 76.8M 445.6G 1280 63 - / 53.7% - -\\nYOLOX-X [21] 99.1M 281.9G 640 58 51.5% / 51.1% - -\\nYOLOv7-E6 97.2M 515.2G 1280 56 56.0% /55.9% 73.5% 61.2%\\nYOLOR-E6 [81] 115.8M 683.2G 1280 45 55.8% / 55.7% 73.4% 61.1%\\nPPYOLOE-X [85] 98.4M 206.6G 640 45 52.2% / 51.9% 69.9% 56.5%\\nYOLOv7-D6 154.7M 806.8G 1280 44 56.6% /56.3% 74.0% 61.8%\\nYOLOv5-X6 (r6.1) [23] 140.7M 839.2G 1280 38 - / 55.0% - -\\nYOLOv7-E6E 151.7M 843.2G 1280 36 56.8% /56.8% 74.4% 62.1%'),\n",
              " Document(id='a8f6e253-4fa6-418c-a7de-93a298c47875', metadata={}, page_content='YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object\\ndetectors\\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\\n1Institute of Information Science, Academia Sinica, Taiwan\\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\\nAbstract\\nYOLOv7 surpasses all known object detectors in both\\nspeed and accuracy in the range from 5 FPS to 160 FPS\\nand has the highest accuracy 56.8% AP among all known')]"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Perform a similarity search in the Pinecone vector store using the specified query\n",
        "# This searches for the top 'k' most relevant documents based on the query\n",
        "docs = docsearch.similarity_search(query, k=3)\n",
        "\n",
        "# Output the retrieved documents that match the similarity criteria\n",
        "docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qihAUwg6g4Yk"
      },
      "source": [
        "## **9. Query the Vector Database**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "498ZuThegv44"
      },
      "outputs": [],
      "source": [
        "# Initialize an instance of the OpenAI language model (LLM)\n",
        "llm = OpenAI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "L841VifyhDg5"
      },
      "outputs": [],
      "source": [
        "# Create a RetrievalQA instance that combines a language model with a retriever\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    # Specify the language model to be used for question answering\n",
        "    llm=llm,\n",
        "\n",
        "    # Define the type of chain to use for processing the data; \"stuff\" indicates a specific method of combining information\n",
        "    chain_type=\"stuff\",\n",
        "\n",
        "    # Use the document searcher as the retriever to fetch relevant documents for the QA process\n",
        "    retriever=docsearch.as_retriever()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhTtD_R1iMzL",
        "outputId": "6f6d6c4e-a88a-4f7d-c59d-1359bee6958e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'query': 'YOLOv7 outperforms which models',\n",
              " 'result': ' YOLOv7 outperforms all known object detectors including YOLOR-D6, YOLOv7-E6, YOLOX-X, YOLOv5-X6, PPYOLOE-X, YOLOv5-X, YOLOR-CSP, YOLOR-CSP-X, YOLOv7-tiny-SiLU, YOLOv7, and YOLOv7-X.'}"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Invoke the question-answering system with the specified query\n",
        "# This processes the query and retrieves an answer using the language model and the document retriever\n",
        "qa.invoke(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gbbyJgFmkjhU",
        "outputId": "08a54b06-2dc6-4723-c05f-4d81c2f1f34f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Rachel Green has a PhD in English from the University of Illinois at Urbana-Champaign and has also completed an MA in English. She has received various grants and awards for her academic achievements, including a Summer Research Grant and a Graduate College Conference Travel Grant from the University of Illinois. She has also presented at conferences and has published in academic journals and books. She currently works as an Associate Professor of English at Butler University in Indianapolis, IN.\n"
          ]
        }
      ],
      "source": [
        "# Define a query string to search for specific information related to Rachel Green's experience\n",
        "query2 = \"Rachel Green Experience\"\n",
        "\n",
        "# Invoke the question-answering system with the defined query to get a response\n",
        "response = qa.invoke(query2)\n",
        "\n",
        "# Extract just the content of the 'result' field from the response dictionary\n",
        "result_content = response.get('result')\n",
        "\n",
        "# Print the extracted result content to the console\n",
        "print(result_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "GznWVur4lPt7",
        "outputId": "d290aae5-9b0f-45dd-bad5-74dfdc28a1fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Prompt: Tell me about yolo v7\n",
            "Answer:  YOLOv7 is an object detection model that has surpassed all known object detectors in terms of speed and accuracy. It has a 56.8% average precision (AP) and can process images at a rate of 5 to 160 frames per second (FPS). It was trained only on the MS COCO dataset, without using any other datasets or pre-trained weights. Comparing YOLOv7 with YOLOR, it has a faster inference speed and a higher detection rate. It also has improvements in AP and parameter and computation reduction compared to YOLOv5. YOLOv7-D6 has a similar inference speed to YOLOR-E6 but with a higher AP by 0.8%.\n",
            "Input Prompt: exit\n",
            "Exiting\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Start an infinite loop to continuously accept user input\n",
        "while True:\n",
        "    try:\n",
        "        # Prompt the user for input and store it in the 'user_input' variable\n",
        "        user_input = input(\"Input Prompt: \")\n",
        "\n",
        "        # Check if the user input is 'exit'\n",
        "        if user_input.lower() == 'exit':\n",
        "            print('Exiting')\n",
        "            # Terminate the program\n",
        "            sys.exit()\n",
        "\n",
        "        # Check if the user input is an empty string\n",
        "        if user_input == '':\n",
        "            # If input is empty, skip to the next iteration of the loop\n",
        "            continue\n",
        "\n",
        "        # Call the question-answering system using the invoke method\n",
        "        result = qa.invoke({'query': user_input})\n",
        "\n",
        "        # Print the answer retrieved from the result, specifically the 'result' field\n",
        "        print(f\"Answer: {result['result']}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any exceptions that occur and print an error message\n",
        "        print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zXYaFw4l167"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
